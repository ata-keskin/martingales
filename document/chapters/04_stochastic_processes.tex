% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Stochastic Processes}\label{chapter:stochastic_processes}

It wouldn't make sense to talk about martingales without introducing stochastic processes first. In standard terminology, a stochastic process is a collection of random variables defined on the same probability space. The indexing set often represents time, and each random variable in the collection corresponds to an outcome at a specific point in that set. These processes are fundamental in understanding how random processes evolve over time. 

Take the example of stock price movement, where each day's stock price is a random variable influenced by a variety of uncertain factors. This sequence of prices forms a stochastic process, describing the stock's behavior. Another instance is the Poisson process, which models events like customer arrivals at a service center. This process captures the randomness in the timing of arrivals, aiding in optimizing resource allocation and enhancing customer service. In physics, Brownian motion characterizes the unpredictable and continuous trajectory followed by particles suspended in a medium due to random collisions with surrounding molecules, which is again modelled as a stochastic process. The theory of stochastic processes is the cornerstone for analysing randomness and building models that mirror real-world uncertainties.

Keeping this in consideration, we aim to build a comprehensive foundation for a theory of stochastic process in Isabelle. Since the definition is so straightforward, it usually suffices to just consider a collection of measurable functions to make formal statements about stochastic processes. There is not much to gain from making an explicit definition on its own. Nonetheless, we must create a framework to discuss stochastic processes that can afterwards be broadened to formalize concepts like adaptedness and predictability. Locales present themselves as the solution we are looking for.

The locale system in Isabelle is useful for managing large formal developments, as it promotes modularity and reusability. It allows us to define generic theorems and structures in one place and then reuse them in multiple contexts without duplicating efforts. For instance, when defining filtered measure spaces in the following section, we will need to have an element act as the de facto bottom element of an index type. Locales allow us to easily fix such an element for this purpose.

We start with the following locale definition.

\begin{isadefinition}
{\small
\begin{lstlisting}[style=isabelle]
locale stochastic_process =
  fixes $M \; t_0$ 
	and $X$ :: "$'b \; :: \; \{$second_countable_topology, linorder_topology$\} \Rightarrow \; 'a \; \Rightarrow \; 'c$"
  assumes random_variable[measurable]: "$\bigwedge i. \; t_0 \le i \implies X \; i \in \texttt{borel\_measurable} \; M$"
\end{lstlisting}
}
\end{isadefinition}

The measure $M$ represents the underlying measure space on which the stochastic process is defined. The index $t_0$ represents the initial point in time beyond which the process $X$ should be defined. As such, this locale formalizes a stochastic process defined on an interval $[t_0, \infty)$. 

We have the following lemmas to introduce ``constant'' stochastic processes.

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
lemma stochastic_process_const_fun:
  assumes "$f \in \texttt{borel\_measurable} \; M$"
  shows "$\texttt{stochastic\_process} \; M \; t_0 \; (\lambda \_. \; f)$"
    using assms by (unfold_locales)

lemma stochastic_process_const:
  shows "$\texttt{stochastic\_process} \; M \; t_0 \; (\lambda i \; \_. \; c \; i)$" 
    by (unfold_locales) simp

\end{lstlisting}
}
\end{isalemma}

For sake of completeness, we also provide the following collection of lemmas which show that stochastic processes are stable under various operations on a vector space.

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
lemma compose:
  assumes "$\bigwedge i. \; t_0 \le i \implies f \; i \in \texttt{borel\_measurable} \; \texttt{borel}$"
  shows "$\texttt{stochastic\_process} \; M \; t_0 \; (\lambda i \; x. \; (f \; i) \; (X \; i \; x))$"
  by (unfold_locales) (intro measurable_compose[OF random_variable assms])
  \end{lstlisting}
}
\end{isalemma}


\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
lemma norm: "$\texttt{stochastic\_process} \; M \; t_0 \; (\lambda i \; x. \; \texttt{norm} \; (X \; i \; x))$" 
  by (fastforce intro: compose)
  \end{lstlisting}
}
\end{isalemma}

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
lemma scaleR_right:
  assumes "$\texttt{stochastic\_process} \; M \; t_0 \; Y$"
  shows "$\texttt{stochastic\_process} \; M \; t_0 \; (\lambda i \; x. \; (Y \; i \; x) \; \cdot_R (X \; i \; x))$"
  using stochastic_process.random_variable[OF assms] random_variable 
  by (unfold_locales) simp

lemma scaleR_right_const_fun: 
  assumes "$f \in \texttt{borel\_measurable} \; M$" 
  shows "$\texttt{stochastic\_process} \; M \; t_0 \; (\lambda i \; x. \; f \; x \cdot_R (X \; i \; x))$" 
  by (unfold_locales) (intro borel_measurable_scaleR assms random_variable)

lemma scaleR_right_const: "$\texttt{stochastic\_process} \; M \; t_0 \; (\lambda i \; x. \; c \; i \cdot_R (X \; i \; x))$"
  by (unfold_locales) simp
\end{lstlisting}
}
\end{isalemma}


\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
lemma add:
  assumes "$\texttt{stochastic\_process} \; M \; t_0 \; Y$"
  shows "$\texttt{stochastic\_process} \; M \; t_0 \; (\lambda i \; x. \; X \; i \; x + Y \; i \; x)$"
  using stochastic_process.random_variable[OF assms] random_variable 
  by (unfold_locales) simp
\end{lstlisting}
}
\end{isalemma}

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
lemma diff:
  assumes "$\texttt{stochastic\_process} \; M \; t_0 \; Y$"
  shows "$\texttt{stochastic\_process} \; M \; t_0 \; (\lambda i \; x. \; X \; i \; x - Y \; i \; x)$"
  using stochastic_process.random_variable[OF assms] random_variable 
  by (unfold_locales) simp
\end{lstlisting}
}
\end{isalemma}

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
lemma uminus: "$\texttt{stochastic\_process} \; M \; t_0 \; (-X)$" using scaleR_right_const[of "$\lambda \texttt{\_}. \; -1$"] 
	by (simp add: fun_Compl_def)
\end{lstlisting}
}
\end{isalemma}

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
lemma partial_sum: "$\texttt{stochastic\_process} \; M \; t_0 \; (\lambda n \; x. \; \sum{i\in\{t_0..<n\}}. \; X \; i \; x)$" 
	by (unfold_locales) simp

lemma partial_sum': "$\texttt{stochastic\_process} \; M \; t_0 \; (\lambda n \; x. \; \sum{i\in\{t_0..n\}}. \; X \; i \; x)$" 
	by (unfold_locales) simp
\end{lstlisting}
}
\end{isalemma}

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]

lemma stochastic_process_sum:
  assumes "$\bigwedge i. \; i \in I \implies \texttt{stochastic\_process} \; M \; t_0 \; (X \; i)$"
  shows "$\texttt{stochastic\_process} \; M \; t_0 \; (\lambda k x. \; \sum i \in I. \; X \; i \; k \; x)$" 
    using assms[THEN stochastic_process.random_variable] by (unfold_locales, auto)

\end{lstlisting}
}
\end{isalemma}

We also specify the following sublocales to easily make statements about discrete-time and continuous-time stochastic processes.

\begin{isadefinition}
{\small
\begin{lstlisting}[style=isabelle]
locale nat_stochastic_process = stochastic_process $M$ "0 :: nat" $X$ for $M \; X$
locale real_stochastic_process = stochastic_process $M$ "0 :: real" $X$ for $M \; X$
\end{lstlisting}
}
\end{isadefinition}

By explicitly designating an element $t_0$ to be the bottom element, we can formalize continuous-time stochastic processes, i.e. $(X_t)_{t \in \mathbb{R}_{\ge 0}}$, without the need for introducing a new type for non-negative real numbers. 

\begin{remark}
Moving forward, we will define the concepts of adaptedness, progressive measurability and predictability. In our formalization, we have introduced analogous lemmas and sublocales for these process varieties as well. To avoid repeating ourselves, we will only reiterate these statements, if the proofs become non-trivial or if the assumptions change.
\end{remark}

Before presenting the remaining process varities, we must introduce the concept of a filtered measure space.

\section{Filtered Measure Spaces}

A filtered measure space is a measure space equipped with a sequence of increasing sub-$\sigma$-algebras, called a \textit{filtration} that represents the accumulation of information over time.

Concretely, let $M$ be a measure space. Assume we have a sequence of sigma-algebras $(F_n)_{n \in \mathbb{N}}$ where 
\[
	F_0 \subseteq F_1 \subseteq F_2 \subseteq \dots
\]
This sequence forms a filtration on $M$. Intuitively, each $F_n$ represents the information available up to time $n$. In general, the index set does not need to be countable. We only need it to be totally ordered, so that two indices are always comparable with one another. In Isabelle, we define the following locale to capture this concept.

\begin{isadefinition}
{\small
\begin{lstlisting}[style=isabelle]
locale filtered_measure = 
  fixes $M \; F$ and $t_0$ :: "$'b \; :: \; \{$second_countable_topology, linorder_topology$\}$"
  assumes subalgebra: "$\bigwedge i. \; t_0 \le i \implies \texttt{subalgebra} \; M \; (F \; i)$"
      and sets_F_mono: "$\bigwedge i \; j. \; t_0 \le i \implies i \le j \implies \texttt{sets} \; (F \; i) \subseteq \texttt{sets} \; (F \; j)$"
\end{lstlisting}

with the predicate \texttt{subalgebra} in \texttt{HOL-Probability.Conditional\_Expectation} defined via

\begin{lstlisting}[style=isabelle]
definition subalgebra::"$'a \; \texttt{measure} \; \Rightarrow 'a \; \texttt{measure} \; \Rightarrow \texttt{bool}$" where
  "$\texttt{subalgebra} \; M \; F = ((\texttt{space} \; F = \texttt{space} \; M) \wedge (\texttt{sets} \; F \subseteq \texttt{sets} \; M))$"
  \end{lstlisting}
}
\end{isadefinition}

\begin{remark}
	In Isabelle the $\texttt{measure}$ type is used to represent both measure spaces and $\sigma$-algebras. The latter is achieved by only considering the underlying $\sigma$-algebra via the projection \texttt{sets}.
\end{remark}

In general, a type with an ordering does not necessarily inhabit a bottom element, i.e. an element that is lesser than any other element. In the next section, we will see how the existence of a bottom element lets us make easy statements about what constitutes an adapted process and what not. From a practical point of view, this is not too much to assume, since all random processes one encounters in the real world must start at some fixed point in time (or at least that assumption can be made for practical purposes).

The keen reader might have noticed that we need a little bit more to define martingales properly. Namely, the sub-$\sigma$-algebras that comprise the filtration $(F_n)_{n \in \mathbb{N}}$ must all be $\sigma$-finite. Otherwise, we can't make use of our lemmas concerning the conditional expectation. We introduce the following locale to adress this issue.

\begin{isadefinition}
{\small
\begin{lstlisting}[style=isabelle]
locale sigma_finite_filtered_measure = filtered_measure +
  assumes sigma_finite: "sigma_finite_subalgebra $M \; (F \; t_0)$"
  \end{lstlisting}
}
\end{isadefinition}
\begin{remark}
	Since we artifically designated an element $t_0$ to represent the least index in consideration, we only need to show $\sigma$-finiteness for the sub-$\sigma$-algebra $F_{t_0}$. $\sigma$-finiteness of all other sub-$\sigma$-algebras follows from the monotonicity of the filtration.
\end{remark}

In order to make the ideas in this section a bit more concrete, we present the following two filtrations as examples.

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
lemma filtered_measure_constant_filtration:
  assumes "$\texttt{subalgebra} \; M \; F$"
  shows "$\texttt{filtered\_measure} \; M \; (\lambda \_. \; F) \; t_0$"
  using assms by (unfold_locales) (auto simp add: subalgebra_def)

sublocale sigma_finite_subalgebra $\subseteq$ constant_filtration: 
		sigma_finite_filtered_measure $M$ "$(\lambda$_. $F$)" $t_0$
  using subalg by (unfold_locales) (auto simp add: subalgebra_def)
\end{lstlisting}
}
\end{isalemma}

If we have some sub-$\sigma$-algebra $F \subseteq \Sigma$, then we can trivially take as our filtration $F_i = F$ for all $i \in [t_0,\infty)$. If we additionally know that we are working with a $\sigma$-finite subalgebra, then this yields a trivial $\sigma$-finite filtration on $M$. This choice of filtration is called a \textbf{constant filtration}. 

\begin{remark}
	In the above lemma, both entries convey the same information. The first one is stated in terms of premises and results, the latter in the language of locales. The notion of a $\sigma$-algebra being a subalgebra is formalized via the predicate \texttt{subalgebra}. Had the formalization been done in the language of locales, we could replace the first statement with an equivalent sublocale relation.
\end{remark}

Preparing for our next example, we introduce a formalization for the notion of a $\sigma$-algebra generated by a family of functions.

\begin{isadefinition}
{\small
\begin{lstlisting}[style=isabelle]
	definition sigma_gen :: "$'a \; \texttt{set} \; \Rightarrow \; 'b \; \texttt{measure} \; \Rightarrow \; ('a \; \Rightarrow \; 'b) \; \texttt{set} \; \Rightarrow \; 'a \;\texttt{measure}$" where
	  "$\texttt{sigma\_gen} \; \Omega \; N \; S \equiv \texttt{sigma} \; \Omega \; (\bigcup f \in S. \; \{(f \;\text{-\textasciigrave} \; A) \; \cap \; \Omega \;\vert\; A. \; A \in N\})$"
  \end{lstlisting}
}
\end{isadefinition}

Given two measure spaces $(V, \mathcal{A})$ and $(W, \mathcal{B})$, it is a well known fact that a function $f : V \rightarrow W$ is measurable, if and only if the generated $\sigma$-algebra $\sigma(f)$ is a subalgebra of $\mathcal{A}$. This result is captured for families of functions in the following lemma.

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
lemma measurable_family_iff_contains_sigma_gen:
  shows "$(S \subseteq M \rightarrow_M N) \longleftrightarrow S \subseteq (\texttt{space}\; M \rightarrow \;\texttt{space}\; N) \wedge \texttt{sigma\_gen} \; (\texttt{space}\; M) \; N \; S \subseteq M$"
\end{lstlisting}
}
\end{isalemma}

Now, we can introduce our more interesting example, the \textbf{natural filtration}.

\begin{isadefinition}
{\small
\begin{lstlisting}[style=isabelle]
definition natural_filtration :: "$'a \; \texttt{measure} \; \Rightarrow \; 'b \; \Rightarrow \; ('b \; \Rightarrow \; 'a \; \Rightarrow \; 'c ) \; \Rightarrow \; 'b \; \Rightarrow \; 'a \; \texttt{measure}$" where
  "$\texttt{natural\_filtration} \; M \; t_0 \; Y = (\lambda t. \; \texttt{sigma\_gen} \; (\texttt{space} \; M) \; \texttt{borel} \; \{Y \; i \; \vert\; i. \; i \in \{t_0..t\}\})$"
\end{lstlisting}
}
\end{isadefinition}
The natural filtration with respect to a stochastic process $Y$ is the filtration generated by all events involving the process up to the time index $t$, i.e. $F_t = \sigma(\{Y_i \; \vert\; i. \; i¸ \le t\})$. Assuming that $Y$ is a stochastic process, i.e. $Y_i$ is $M$-measurable for all $i \ge t_0$, the definition indeed provides a filtration. The following sublocale relation formalizes this.

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
sublocale stochastic_process $\subseteq$ filtered_measure_natural_filtration: 
	filtered_measure $M$ "natural_filtration $M$ $t_0$ $X$" $t_0$
    by (unfold_locales) (intro subalgebra_natural_filtration, 
						 simp only: sets_natural_filtration, 
						 intro sigma_sets_subseteq, force) 
\end{lstlisting}
}
\end{isalemma}

The natural filtration contains information concerning the process's past behavior at each point in time. The natural filtration is essentially the simplest filtration for studying a process. However, the natural filtration is not always $\sigma$-finite. In order to show that the natural filtration constitutes a sigma finite filtered measure, we need to provide a countable exhausting set in the preimage of $X_{t_0}$.

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
lemma (in sigma_finite_measure) sigma_finite_filtered_measure_natural_filtration:
  assumes "$\texttt{stochastic\_process} \; M \; t_0 \; X$"
      and exhausting_set: "$\texttt{countable} \; A$" "$(\bigcup A) = \texttt{space} \; M$" 
		  "$\bigwedge a. \; a \in A \implies \texttt{emeasure} \; M \; a \neq \infty$" 
		  "$\bigwedge a. \; a \in A \implies \exists b \in \texttt{borel}. \; a = ((X \; t_0) \; \text{-\textasciigrave} \; b) \; \cap \; \texttt{space} \; M$"
    shows "$\texttt{sigma\_finite\_filtered\_measure} \; M \; (\texttt{natural\_filtration} \; M \; t_0 \; X) \; t_0$"
\end{lstlisting}
}
\end{isalemma}

This concludes our development of filtered measure spaces.

\section{Adapted Processes}

A stochastic process $X$ is called adapted to a filtration $(F_t)_{t \in [t_0, \infty)}$ if, for every index $t \ge t_0$, the random variable $X_t$ is measurable with respect to the $\sigma$-algebra $F_t$. This means that the value of $X_t$ depends only on the information available up to time $t$. In other words, the process ``adapts'' to the information in a way that it cannot anticipate future values based on events that have not occurred yet. We introduce the following locale.

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
locale adapted_process = filtered_measure $M$ $F$ $t_0$ for $M$ $F$ $t_0$ 
	and $X$ :: "$\_ \; \Rightarrow \; \_ \; \Rightarrow \; \_ :: \; \{$second_countable_topology, banach$\}$" $+$
  assumes adapted[measurable]: "$\bigwedge i. \; t_0 \le i \implies X \; i \in \texttt{borel\_measurable} \; (F \; i)$"
\end{lstlisting}
}
\end{isalemma}

The properties we have shown concerning stochastic processes also hold for adapted processes. Although in some cases, for example in the following statement, we need to modify the measurability assumptions we make. Here, we see how constraining ourselves to an index set bounded from below helps make the assumption simpler.

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
lemma (in filtered_measure) adapted_process_const_fun:
  assumes "$f \in \texttt{borel\_measurable} \; (F \; t_0)$"
  shows "$\texttt{adapted\_process} \; M \; F \; t_0 \; (\lambda \_. \; f)$"
\end{lstlisting}
}
\end{isalemma}

Furthermore, in the presence of a discrete index set, we have the following additional lemma concerning partial sums.

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
lemma (in nat_adapted_process) partial_sum_Suc: 
	"$\texttt{nat\_adapted\_process} \; M \; F \; (\lambda n \; x. \sum i<n. \; X \; (\texttt{Suc} \; i) \; x)$" 
proof (unfold_locales)
  fix $i$
  have "$X \; j \in \texttt{borel\_measurable} \; (F \; i)$" if "$j \le i$" for $j$ using that adaptedD by blast
  thus "$(\lambda x. \; \sum i<n. \; X \; (\texttt{Suc} \; i) \; x) \in \texttt{borel\_measurable} \; (F \; i)$" by auto
qed
\end{lstlisting}
}
\end{isalemma}

An adapted process is necessarily a stochastic process. This follows directly from the fact that $F_t \subseteq \Sigma$ for all $t \ge t_0$.

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
sublocale adapted_process $\subseteq$ stochastic_process 
  using measurable_from_subalg subalgebra adapted by (unfold_locales) blast

sublocale nat_adapted_process $\subseteq$ nat_stochastic_process ..
sublocale real_adapted_process $\subseteq$ real_stochastic_process ..
\end{lstlisting}
}
\end{isalemma}


In the other direction, a stochastic process is always adapted to the natural filtration it generates.

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
sublocale stochastic_process $\subseteq$ adapted_natural: 
	adapted_process $M$ "natural_filtration $M$ $t_0$ $X$" $t_0$ $X$ 
  by (unfold_locales) (auto simp add: natural_filtration_def 
  					   intro: random_variable measurable_sigma_gen) 
\end{lstlisting}
}
\end{isalemma}

Adapted processes are cruicial for defining martingales. A martingale is by definition an adapted process. In the following section, we will explore progressively measurable processes, even though they are not directly relevant to our formalization of martingales. This serves two purposes: first, to replicate the corresponding results on \textsf{mathlib}, and second, to establish a solid foundation for future projects to build upon.

\section{Progressively Measurable Processes}

The definition of a progressively measurable process is more intricate.

\begin{definition}
	Let $F$ be a filtration of the measure space $M$. A stochastic process $X$ is called progressively measurable (or simply \textit{progressive}) if, for every index $t \ge t_0$, the map $[t_0, t] \times \Omega \rightarrow E$ defined by $(i, w) \mapsto X_i(w)$ is measurable with respect to the $\sigma$-algebra $\mathcal{B}([t_0, t]) \otimes F_t$. Here $\mathcal{B}([t_0, t])$ denotes the Borel $\sigma$-algebra on $[t_0, t]$ induced by the order topology.
\end{definition}

The formalized version is as follows.

\begin{isadefinition}
{\small
\begin{lstlisting}[style=isabelle]
locale progressive_process = filtered_measure $M$ $F$ $t_0$ for $M$ $F$ $t_0$
	and $X$ :: "$\_ \; \Rightarrow \; \_ \; \Rightarrow \; \_ :: \; \{$second_countable_topology, banach$\}$" $+$
  assumes progressive[measurable]: "$\bigwedge t. \; t_0 \le t$
	$\implies (\lambda (i, x). \; X \; (\min \; t \; i) \; x) \in \texttt{borel\_measurable} \; (\texttt{restrict\_space} \; \texttt{borel} \; \{t_0..t\} \otimes_M (F \; t))$"
\end{lstlisting}
}
\end{isadefinition}

Formalizing this concept also demonstrates its intricacies. In order to constrain the domain of the map $(i, w) \mapsto X_i(w)$ to the index set $[t_0, t]$, we compose it with the function $\min(t, \cdot)$. This makes sure that the first argument of the map always lies in the set $[t_0, t]$, effectively making it a map $[t_0, t] \times \Omega \rightarrow E$. 

Notice that the measurability assumption we make here is on the entire map $(i, w) \mapsto X_i(w)$ instead of being ``pointwise'' as in the previous two sections. As a side effect, the stochastic process defined by $X_i = c(i)$ for some $c : [t_0, \infty) \rightarrow E$ is progressively measurable, only if the function $c$ is Borel measurable. Previously, this assumption was not required.

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
lemma (in filtered_measure) progressive_process_const:
  assumes "$c \in \texttt{borel\_measurable} \; \texttt{borel}$"
  shows "$\texttt{progressive\_process} \; M \; F \; t_0 \; (\lambda i \; \_. \; c \; i)$"
    using assms by (unfold_locales) 
				   (auto simp add: measurable_split_conv 
					intro!: measurable_compose[OF measurable_fst] 
					measurable_restrict_space1)
  \end{lstlisting}
}
\end{isalemma}

Similarly, we must modify the premise of the lemma \texttt{compose} in order to reflect this change.

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
lemma compose:
  assumes "$(\lambda (i, x). \; f \; i \; x) \in \texttt{borel\_measurable} \; \texttt{borel}$"
  shows "$\texttt{progressive\_process} \; M \; F \; t_0 \; (\lambda i \; x. \; (f \; i) \; (X \; i \; x))$"
  $\dots$
  \end{lstlisting}
}
\end{isalemma}

A progressively measurable process is necessarily adapted. The proof is trivial and arises from the fact that the injection $y \mapsto (t, y)$ is measurable as a function $\Omega \rightarrow [t_0, t] \times \Omega$ for fixed $t \ge t_0$.

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
sublocale progressive_process $\subseteq$ adapted_process 
  using measurable_compose_rev[OF progressive measurable_Pair1'] 
  unfolding prod.case by (unfold_locales) simp
\end{lstlisting}
}
\end{isalemma}

On a more interesting note, progressive measurability is equivalent to adaptedness in the discrete setting. The following lemma demonstrates this.

\begin{lemma}
	Let $(X_i)_{i \in \mathbb{N}}$ be an adapted process with respect to the filtration $(F_i)_{i \in \mathbb{N}}$. Then it is also progressively measurable.
\end{lemma}
\begin{proof}
	Let $S$ be an open set in $E$. Then $X_j^{-1}(S) \in F_i$ for all $j \le i \in \mathbb{N}$, since $(X_i)_{i \in \mathbb{N}}$ is adapted by assumption. Let $\psi : \{0,\dots,i\} \times \Omega \rightarrow E$ with $\psi(j,x) = X_j(x)$. Then, we have
	\[
		\psi^{-1}(S) \cap \{j\} \times \Omega = \{j\} \times X_j^{-1}(S) \in \mathcal{B}(\{0,\dots,i\}) \otimes F_i
	\]
	since the order topology on $\mathbb{N}$ is discrete. Furthermore
	\[
		\psi^{-1}(S) = \bigcup_{j \le i} \psi^{-1}(S) \cap \{j\} \times \Omega 
	\]
	Since the set $\{0,\dots,i\}$ is countable, it follows that $\psi^{-1}(S) \in \mathcal{B}(\{0,\dots,i\}) \otimes F_i$, since it is expressable as the union of a countable family of measurable sets.
\end{proof}

Subsequently we express this fact in the language of locales.

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
sublocale nat_adapted_process $\subseteq$ nat_progressive_process
  $\dots$
\end{lstlisting}
}
\end{isalemma}

Now comes the most challenging portion of this chapter.

\section{Predictable Processes}

Before defining predictable processes in full generality, we will discuss them in the discrete setting, where the definition is easier to grasp.

\begin{definition}
	A discrete-time stochastic process $(X_i)_{i\in\mathbb{N}}$ is called predictable with respect to a filtration $(F_i)_{i\in\mathbb{N}}$, if $X_{i + 1}$ is $F_i$-measurable for all $i \in \mathbb{N}$.
\end{definition}



