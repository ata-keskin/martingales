% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Martingales}\label{chapter:martingales}

In this section we will introduce and discuss martingales, the namesake of our thesis. Originally referring to a system of betting strategies, martingales have evolved far beyond their gambling origins and have found profound applications in various fields, including finance, probability theory, and statistical analysis. Our formalization aims for a high level of generality while maintaining clarity and simplicity, making it easier for future formalization efforts to build upon our foundation. The results formalized in this chapter can be found in the theory file \texttt{Martingale.Martingale} \cite{Keskin_A_Formalization_of_2023}.

\section{Fundamentals}

We start with the definition of a martingale.

\begin{definition}
	Let $(F_t)_{t \in [t_0,\infty)}$ be a filtration of the measure space $M$. A stochastic process $(X_t)_{t \in [t_0,\infty)}$ taking values in a Banach space $(E, \lVert \cdot \rVert)$ is a \textit{martingale with respect to the filtration $(F_t)_{t \in [t_0,\infty)}$} if the following conditions hold
	\begin{enumerate}
	\item $(X_t)_{t \in [t_0,\infty)}$ is adapted to the filtration $(F_t)_{[t_0,\infty)}$,
	\item $X_t \in L^1(E)$ for all $t \in [t_0, \infty)$,
	\item $X_s = \mathbb{E}(X_t \;\vert\; F_s)$ $\mu$-a.e. for all $s,t \in [t_0,\infty)$ with $s \le t$.
	\end{enumerate}
	Replacing ``$=$'' in the third condition with ``$\le$'' or ``$\ge$'' gives rise to the definition of a sub- or supermartingale, respectively.
\end{definition}

\begin{remark}
 In addition to what we've discussed in the last chapter, we have introduced the locale \texttt{sigma\_finite\_adapted\_process} which combines the locale \texttt{adapted\_process} with the locale \texttt{sigma\_finite\_filtered\_measure}. Without this additional restriction, we can't use the operator \texttt{cond\_exp}. Similary, the locale \texttt{sigma\_finite\_adapted\_process\-\_order} places a restriction on the Banach space $(E, \lVert \cdot \rVert)$, asserting the existence of an ordering compatible with scalar multiplication. Finally, the locale \texttt{sigma\_finite\_adapted\-\_process\_linorder} further mandates that this ordering be total. We have also introduced locales for discrete-time and continuous-time counterparts.
\end{remark}

Using these additional definitions we introduce the following locale which formalizes martingales.

\begin{isadefinition}
{\small
\begin{lstlisting}[style=isabelle]
locale martingale = sigma_finite_adapted_process +
  assumes integrable: "$\bigwedge i. \; t_0 \le i \implies \texttt{integrable} \; M \; (X \; i)$"
      and martingale_property: "$\bigwedge i \; j. \; t_0 \le i \implies i \le j$
	  $\implies $AE$ \; x \; $in$ \; M. \; X \; i \; x = \texttt{cond\_exp} \; M \; (F \; i) \; (X \; j) \; x$"
\end{lstlisting}
}
\end{isadefinition}

Locales for submartingales and supermartingales are introduced similarly.

\begin{isadefinition}
{\small
\begin{lstlisting}[style=isabelle]
locale submartingale = sigma_finite_adapted_process_order +
  assumes integrable: "$\bigwedge i. \; t_0 \le i \implies \texttt{integrable} \; M \; (X \; i)$"
      and submartingale_property: "$\bigwedge i \; j. \; t_0 \le i \implies i \le j$
	  $\implies $AE$ \; x \; $in$ \; M. \; X \; i \; x \le \texttt{cond\_exp} \; M \; (F \; i) \; (X \; j) \; x$"
\end{lstlisting}
}
\end{isadefinition}

\begin{isadefinition}
{\small
\begin{lstlisting}[style=isabelle]
locale supermartingale = sigma_finite_adapted_process_order +
  assumes integrable: "$\bigwedge i. \; t_0 \le i \implies \texttt{integrable} \; M \; (X \; i)$"
      and supermartingale_property: "$\bigwedge i \; j. \; t_0 \le i \implies i \le j$
	  $\implies $AE$ \; x \; $in$ \; M. \; X \; i \; x \ge \texttt{cond\_exp} \; M \; (F \; i) \; (X \; j) \; x$"
\end{lstlisting}
}
\end{isadefinition}

Any stochastic process that is both a submartingale and a supermartingale is a martingale. Conversely, every martingale is also a submartingale and a supermartingale if there exists an ordering on the Banach space $E$. In anticipation of this result, we introduce the following locale.

\begin{isadefinition}
{\small
\begin{lstlisting}[style=isabelle]
locale martingale_order = martingale $M$ $F$ $t_0$ $X$ for $M$ $F$ $t_0$
	and $X$ :: "_ $\Rightarrow$ _ $\Rightarrow$ _ :: {order_topology, ordered_real_vector}"
\end{lstlisting}
}
\end{isadefinition}

Using thise locale we can state the following lemma which formalizes this fact.

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
lemma martingale_iff: 
  shows "$\texttt{martingale} \; M \; F \; t_0 \; X \longleftrightarrow \texttt{submartingale} \; M \; F \; t_0 \; X \; \wedge \; \texttt{supermartingale} \; M \; F \; t_0 \; X$"
\end{lstlisting}
}
\end{isalemma}

Additionally, we have included lemmas for introducing martingales in simple cases. For $f \in L^1(E)$ and $F_{t_0}$-measurable, the constant stochastic process defined by $X_t = f$ is a martingale. The following lemma reflects this.

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
lemma (in sigma_finite_filtered_measure) martingale_const_fun:  
  assumes "$\texttt{integrable} \; M \; f$" "$f \in \texttt{borel\_measurable} \; (F \; t_0)$"
  shows "$\texttt{martingale} \; M \; F \; t_0 \; (\lambda \_. \; f)$"
\end{lstlisting}
}
\end{isalemma}

The statements below follow directly.

\begin{isacorollary}
{\small
\begin{lstlisting}[style=isabelle]
corollary (in sigma_finite_filtered_measure) martingale_zero: 
	"$\texttt{martingale} \; M \; F \; t_0 \; (\lambda \_ \; \_. \; 0)$" by fastforce

corollary (in finite_filtered_measure) martingale_const: 
	"$\texttt{martingale} \; M \; F \; t_0 \; (\lambda \_ \; \_. \; c)$" by fastforce
\end{lstlisting}
}
\end{isacorollary}

Using our development of the conditional expectation operator, we have the following corollary.

\begin{corollary}
	The stochastic process defined by $X_t = \mathbb{E}(f \;\vert\; F_t)$ for $f \in L^1(E)$ is also a martingale. 
\end{corollary}
\begin{proof}
	This follows from the tower property of the conditional expectation.
\end{proof}

\section{Basic Operations and Alternative Characterizations}

First and foremost, we will discuss elementary properties of martingales, submartingales and supermartingales. 

\begin{lemma}
	Let $(X_t)_{t \in [t_0,\infty)}$ be a martingale with respect to a filtration $(F_t)_{t \in [t_0,\infty)}$. We have
	\begin{enumerate}
	\item[$\bullet$] Let $c \in \mathbb{R}$. Then $(c \cdot X_t)_{t \in [t_0,\infty)}$ is also a martingale.
	\item[$\bullet$] Let $(Y_t)_{t \in [t_0,\infty)}$ be another martingale with respect to the same filtration $(F_t)_{t \in [t_0,\infty)}$. Then $(X_t + Y_t)_{t \in [t_0,\infty)}$ and $(X_t - Y_t)_{t \in [t_0,\infty)}$ are also martingales.
	\end{enumerate}
\end{lemma}
\begin{proof}
	The statements follow using the linearity of the conditional expectation and the Bochner-integral.
\end{proof}

Similarly, we have the following statement for submartingales.

\begin{lemma}
	Let $(X_t)_{t \in [t_0,\infty)}$ be a submartingale with respect to a filtration $(F_t)_{t \in [t_0,\infty)}$. We have
	\begin{enumerate}
	\item[$\bullet$] Let $c \in \mathbb{R}$ with $c \ge 0$. Then $(c \cdot X_t)_{t \in [t_0,\infty)}$ is also a submartingale.
	\item[$\bullet$] Let $c \in \mathbb{R}$ with $c \le 0$. Then $(c \cdot X_t)_{t \in [t_0,\infty)}$ is a supermartingale.
	\item[$\bullet$] Let $(Y_t)_{t \in [t_0,\infty)}$ be another submartingale with respect to the same filtration $(F_t)_{t \in [t_0,\infty)}$. Then $(X_t + Y_t)_{t \in [t_0,\infty)}$ is also a submartingale.
	\end{enumerate}
\end{lemma}
\begin{proof}
	These statements also follow using the linearity of the conditional expectation and the Bochner-integral.
\end{proof}

\begin{remark}
In the lemma above we can exchange ``submartingale'' with ``supermartingale'' and the results are still valid.
\end{remark}

Going forward, we use the martingale property and the characterization of the conditional expectation to show the following lemma.

\begin{lemma}
	Let $(X_t)_{t \in [t_0,\infty)}$ be a martingale with respect to a filtration $(F_t)_{t \in [t_0,\infty)}$. Let $i,j \in [t_0,\infty)$ with $i \le j$ and $A \in F_i$. Then
	\[
		\int_A X_i \; \textrm{d}\mu = \int_A X_j \; \textrm{d}\mu
	\]
\end{lemma}

This lemma already shows us the intuition behind the definition of a martingale. Let $A$ be a set which is measurable at time $i$, i.e. some property of the process which we can inspect at time $i$. The average value that the process has on this set at time $i$ is equal to the average value it will have on the same set at a future time $j$. Essentially, this is the reason why martingales are employed for modeling fair games that incorporate an element of chance. 

Similarly, for submartingales we have the following lemmas.

\begin{lemma}
	Let $(X_t)_{t \in [t_0,\infty)}$ be a submartingale with respect to a filtration $(F_t)_{t \in [t_0,\infty)}$. Let $i,j \in [t_0,\infty)$ with $i \le j$ and $A \in F_i$. Then
	\[
		\int_A X_i \; \textrm{d}\mu \le \int_A X_j \; \textrm{d}\mu
	\]
\end{lemma}

Replacing ``$\le$'' with ``$\ge$'' gives a corresponding introduction lemma for supermartingales.

In this case, the intuition is similar. The average value of a submartingale on a set which is measurable at time $i$ is less than or equal to the average value it will take on the same set at a future time $j$. The case for a supermartingale is analogous. Here is a simple example illustrating this concept.

\begin{example}
Consider a coin-tossing game, where the coin lands on heads with probability $p \in [0,1]$. Assume that the gambler wins a fixed amount $c > 0$ on a heads outcome and loses the same amount $c$ on a tails outcome. Let $(X_n)_{n \in \mathbb{N}}$ be a stochastic process, where $X_n$ denotes the gambler's fortune after the $n$-th coin toss. Then, we have the following three cases.
\begin{enumerate}
\item If $p = \frac{1}{2}$, it means the coin is fair and has an equal chance of landing heads or tails. In this case, the gambler, on average, neither wins nor loses money over time. The expected value of the gambler's fortune stays the same over time. Therefore, $(X_n)_{n \in \mathbb{N}}$ is a martingale.
\item If $p \ge \frac{1}{2}$, it means the coin is biased in favor of heads. In this case, the gambler is more likely to win money on each bet. Over time, the gambler's fortune tends to increase on average. Therefore, $(X_n)_{n \in \mathbb{N}}$ is a submartingale.
\item If $p \le \frac{1}{2}$, it means the coin is biased in favor of tails. In this scenario, the gambler is more likely to lose money on each bet. Over time, the gambler's fortune decreases on average. Therefore, $(X_n)_{n \in \mathbb{N}}$ is a supermartingale.
\end{enumerate}
\end{example}

The property discussed above is of such fundamental significance that it can be employed to characterize martingales, submartingales, and supermartingales. We present the formal statement first, followed by a subsequent discussion of the proof idea.

\begin{lemma}
	Let $(X_t)_{t \in [t_0,\infty)}$ be an adapted process with respect to the filtration $(F_t)_{t \in [t_0,\infty)}$ consisting of Bochner-integrable random variables.
  Assume 
  \[
  \normalfont \int_A X_i \;\textrm{d}\mu = \int_A X_j \;\textrm{d}\mu
  \] 
  for all $A \in F_i$ and for all $i,j \in [t_0,\infty)$ with $i \le j$. Then the process $(X_t)_{t \in [t_0,\infty)}$ is a martingale.
\end{lemma}
\begin{proof}
Using the defining property of the conditional expectation, the second assumption can be restated as
\[
	\int_A X_i \;\textrm{d}\mu = \int_A \texttt{cond\_exp} \; M \; F_i \; X_j \;\textrm{d}\mu
\]
Applying the lemma on the uniqueness of densities (\ref{cor:density_unique}) and the fact that both functions are $F_i$-measurable, we get
\[
	\quad X_i = \texttt{cond\_exp} \; M \; F_i \; X_j \;\; \mu\vert_{F_i}\textrm{-a.e.}
\]
The statement follows from the fact that $F_i \subseteq \Sigma$, i.e. all $\mu\vert_{F_i}$-null sets are $\mu$-null sets. 
\end{proof}

Analogously, we have the following introduction lemma for submartingales.

\begin{lemma}
	Let $(X_t)_{t \in [t_0,\infty)}$ be an adapted process with respect to the filtration $(F_t)_{t \in [t_0,\infty)}$ consisting of Bochner-integrable random variables.
  Assume 
  \[
  \normalfont \int_A X_i \;\textrm{d}\mu \le \int_A X_j \;\textrm{d}\mu
  \] 
  for all $A \in F_i$ and for all $i,j \in [t_0,\infty)$ with $i \le j$. Then the process $(X_t)_{t \in [t_0,\infty)}$ is a submartingale.
\end{lemma}

The idea of the proof is the same. To prevent redundancy in our formalization, we've demonstrated the lemma for supermartingales\footnote{\texttt{Martingale.supermartingale\_of\_set\_integral\_ge}} by equivalently showing that the stochastic process $(-X_t)_{t \in [t_0,\infty)}$ is a submartingale using the lemma above\footnote{\texttt{Martingale.submartingale\_of\_set\_integral\_le}}. This is easily achieved using the locale system.

Another way to characterize martingales is by examining the conditional expectation of the difference between the process's values at different points in time. We have the following lemmas to address this.

\begin{lemma}
  Let $(X_n)_{n \in \mathbb{N}}$ be an adapted process with respect to the filtration $(F_n)_{n \in \mathbb{N}}$ consisting of Bochner-integrable random variables. Assume 
  \[
  \normalfont \texttt{cond\_exp} \; M \; (F_i) \; (X_{i+1} - X_i) = 0 \quad\mu\textrm{-a.e.}
  \] 
  Then the process $(X_n)_{n \in \mathbb{N}}$ is a martingale.
\end{lemma}

This characterization also has an informal interpretation. The expected future value of a martingale does not change, when we restrict it to those events that we can measure right now. In a similar fashion, the value of a submartingale is expected to increase and the value of a supermartingale is expected to decrease as time progresses.

\section{Discrete-Time Martingales}

Discrete-time martingales are widely used to model processes that evolve over a sequence of discrete time steps while satisfying the martingale property, such as financial asset prices, random walks, and stochastic games. We define the following locales.

\begin{isadefinition}
{\small
\begin{lstlisting}[style=isabelle]
locale nat_martingale = martingale $M$ $F$ "$0 :: \; \texttt{nat}$" $X$ for $M$ $F$ $X$
locale nat_submartingale = submartingale $M$ $F$ "$0 :: \; \texttt{nat}$" $X$ for $M$ $F$ $X$
locale nat_supermartingale = supermartingale $M$ $F$ "$0 :: \; \texttt{nat}$" $X$ for $M$ $F$ $X$
\end{lstlisting}
}
\end{isadefinition}

Many of the statements we have made above, can be simplified when the indexing set is the natural numbers. Given a point in time $i \in \mathbb{N}$, it suffices to consider the successor $i + 1$, instead of all future times $j \ge i$. This can be stated as follows.

\begin{lemma}
	Let $(F_n)_{n \in \mathbb{N}}$ be a filtration of the measure space $M$. A stochastic process $(X_n)_{n \in \mathbb{N}}$ which is adapted to the filtration $(F_n)_{n \in \mathbb{N}}$ is a martingale, if and only if it is integrable at all time indices $n \in \mathbb{N}$ and $X_n = \mathbb{E}(X_{n + 1} \;\vert\; F_n)$ $\mu$-a.e. for all $n \in \mathbb{N}$.
\end{lemma}
\begin{proof}
	The ``only if'' part is evident. For the other direction, let $n, m \in \mathbb{N}$ with $n \le m$. We will show that $X_n = \mathbb{E}(X_m \;\vert\; F_n)$ holds $\mu$-a.e. via induction on the difference $k := m - n$. 
	\begin{enumerate}
	\item[$\bullet$] For the induction basis, assume $k = 0$. Hence $n = m$. Since the process is adapted by our assumption, we have $X_n = \mathbb{E}(X_n \;\vert\; F_n) = \mathbb{E}(X_m \;\vert\; F_n)$.
	
	\item[$\bullet$] For the induction step, let $k + 1 = m - n$, and assume $X_i = \mathbb{E}(X_j \;\vert\; F_i)$ for all $i, j \in \mathbb{N}$ such that $k = j - i$ and $i \le j$. Using the fact that $k = m - (n + 1) \ge 0$, we have
	\[
		X_{n + 1} = \mathbb{E}(X_m \;\vert\; F_{n + 1}) \;\;\mu\textrm{-a.e.}
	\]
	We take the conditonal expectation of both sides with respect to the sub-$\sigma$-algebra $F_n$.
	\[
		\mathbb{E}(X_{n + 1} \;\vert\; F_n) =  \mathbb{E}(\mathbb{E}(X_m \;\vert\; F_{n + 1}) \;\vert\; F_n) \;\;\mu\textrm{-a.e.}
	\]
	Using the tower propery, we have
	\[
		\mathbb{E}(X_{n + 1} \;\vert\; F_n) =  \mathbb{E}(X_m \;\vert\; F_n) \;\;\mu\textrm{-a.e.}
	\]
	Finally, we use our assumption to get
	\[
		X_n =  \mathbb{E}(X_m \;\vert\; F_n) \;\;\mu\textrm{-a.e.}
	\]	
	which completes the proof by induction.
	\end{enumerate}
\end{proof}

The same proof idea can be used to show the statement for submartingales (resp. supermartingales) by replacing ``$=$'' with ``$\le$'' (resp. ``$\ge$'') and using the monotonicity of the conditional expectation. We use this characterization to introduce the following introduction lemmas for discrete-time martingales. Analogous statements hold for submartingales and supermartingales as well:

\begin{lemma}
  Let $(X_n)_{n \in \mathbb{N}}$ be an adapted process with respect to the filtration $(F_n)_{n \in \mathbb{N}}$ consisting of Bochner-integrable random variables. Assume 
  \[
  \normalfont X_i = \texttt{cond\_exp} \; M \; (F_i) \; X_{i+1} \quad\mu\textrm{-a.e.} 
  \]
  for all $i \in \mathbb{N}$. Then the process $(X_n)_{n \in \mathbb{N}}$ is a martingale.
\end{lemma}

We can express the alternative characterizations in the previous subsections this way as well:

\begin{lemma}
  Let $(X_n)_{n \in \mathbb{N}}$ be an adapted process with respect to the filtration $(F_n)_{n \in \mathbb{N}}$ consisting of Bochner-integrable random variables. 
  Assume 
  \[
  \normalfont \int_A X_i \;\textrm{d}\mu = \int_A X_{i+1} \;\textrm{d}\mu
  \] 
  for all $A \in F_i$ for all $i \in \mathbb{N}$. Then the process $(X_n)_{n \in \mathbb{N}}$ is a martingale.
\end{lemma}

\begin{lemma}
  Let $(X_n)_{n \in \mathbb{N}}$ be an adapted process with respect to the filtration $(F_n)_{n \in \mathbb{N}}$ consisting of Bochner-integrable random variables. Assume 
  \[
  \normalfont \texttt{cond\_exp} \; M \; F_i \; (X_{i+1} - X_i) = 0 \quad\mu\textrm{-a.e.}
  \] 
  for all $i \in \mathbb{N}$. Then the process $(X_n)_{n \in \mathbb{N}}$ is a martingale.
\end{lemma}

Lastly, we will discuss the behavior of discrete-time martingales, under the additional assumption that they are predictable. At the end of the preceeding chapter, we had shown that a discrete-time $(X_n)_{n \in \mathbb{N}}$ process is predictable, if and only if the time shifted process $(X_{n + 1})_{n \in \mathbb{N}}$ is an adapted process. That is, $X_{n+1}$ is $F_n$ measurable for all $n \in \mathbb{N}$. Under this additional assumption the martingale property becomes trivial, i.e.
\[
	X_n = \mathbb{E}(X_{n+1} \;\vert\; F_n) = X_{n+1} \;\;\mu\textrm{-a.e.}
\]
By induction, we have $X_n = X_0$ $\mu$-a.e. Hence, a predictable martingale must be constant. The formalized statement is as follows.

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
lemma (in nat_martingale) predictable_const:
  assumes "$\texttt{nat\_predictable\_process} \; M \; F \; X$"
  shows "AE$ \; x \; $in$ \; M. \; X \; i \; x = X \; j \; x$"
\end{lstlisting}
}
\end{isalemma}

In the same vein, a predictable submartingale must be monotonically increasing and a predictable supermartingale must be monotonically decreasing:

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
lemma (in nat_submartingale) predictable_mono:
  assumes "$\texttt{nat\_predictable\_process} \; M \; F \; X$" "$i \le j$"
  shows "AE$ \; x \; $in$ \; M. \; X \; i \; x \le X \; j \; x$"
\end{lstlisting}
}
\end{isalemma}

\begin{isalemma}
{\small
\begin{lstlisting}[style=isabelle]
lemma (in nat_supermartingale) predictable_mono:
  assumes "$\texttt{nat\_predictable\_process} \; M \; F \; X$" "$i \le j$"
  shows "AE$ \; x \; $in$ \; M. \; X \; i \; x \ge X \; j \; x$"
\end{lstlisting}
}
\end{isalemma}

This wraps up our formalization of martingales in Isabelle. As it was out of the scope of this thesis, we have not formalized any major results concerning martingales. That being said, we hold a strong belief that the groundwork we've laid here will provide an excellent foundation for future formalization endeavors.