% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Conditional Expectation in Banach Spaces}\label{chapter:conditional_expectation}

Conditional expectation extends the concept of expected value to situations where we have additional information about the outcomes. In a discrete setting, i.e. when the range of the random variables in question is countable, the setup is quite simple. 
\par Let $(\Omega, \mathcal{F}, \mu)$ be a measure space. Let $E$ be a complete normed vector space, i.e. a Banach space, and $S \subseteq E$ be some countable subset. Let $X : \Omega \rightarrow S$ be an integrable random variable and an event $A \in \mathcal{F}$ with $\mu(A) < \infty$. The conditional expectation of $X$ given $A$, denoted as $\mathbb{E}(X \vert A)$, represents the expected value of $X$ given that $A$ occurs. In this simple case, we can directly define the conditional expectation as:
\[
	\mathbb{E}(X \vert A) = \sum_{w \in S} \frac{\mu(\{X = w\} \cap A)}{\mu(A)} \cdot w
\]
Of course, this definition only makes sense if the value on the right hand side is finite and $\mu(A) \neq 0$. Defined this way, the conditional expectation satisfies the following equality
\begin{align*}
	\int_A X \; \textrm{d} \mu &= \sum_{w \in S} \mu(\{\mathbf{1}_A \cdot X = w\}) \cdot w\\
	&= \mu(A) \cdot \mathbb{E}(X \vert A) \\
	&= \int_A \mathbb{E}(X \vert A) \; \textrm{d} \mu
\end{align*}

\begin{remark}
	We use the notation ``$c \; \cdot \; w$'' to denote the scalar multiplication of $c \in \mathbb{R}$ and $w \in E$. When $E = \mathbb{R}$, it is just the standard multiplication on $\mathbb{R}$. 
\end{remark}

This observation motivates us to generalize the definition of conditional expectation to take into account not just a single event, but a collection of events. Fix $X : \Omega \rightarrow E$. Given a sub-$\sigma$-algebra $\mathcal{H} \subseteq \mathcal{F}$, we call an $\mathcal{H}$-measurable function $g : \Omega \rightarrow E$ a conditional expectation of $X$ with respect to the sub-$\sigma$-algebra $\mathcal{H}$ , denoted as $\mathbb{E}(X \vert \mathcal{H})$, if the following equality holds for all $A \in \mathcal{H}$

\[
	\int_A X \; \textrm{d} \mu = \int_A g \; \textrm{d} \mu
\]

In the case that $E = \mathbb{R}$, it is straightforward to show that such a function $g$ always exists (via Radon-Nikodym), and is unique up to a $\mu$-null set. Notice that $\mathbb{E}(X \vert \mathcal{H})$ is a function $\Omega \rightarrow E$, as opposed to some value in $E$.

The suitable setting for defining the conditional expectation is when the sub-$\sigma$-algebra $\mathcal{H}$ gives rise to a $\sigma$-finite measure space. This is the case when $\mu\vert_\mathcal{H}$, the restriction of $\mu$ to $\mathcal{H}$ is a $\sigma$-finite measure. To see what goes wrong, consider the trivial sub-$\sigma$-algebra $\{\varnothing, \Omega\}$. A function which is measurable with respect to this $\sigma$-algebra is necessarily constant. Therefore, if $\mu(\Omega) = \infty$, no conditional expectation can exist, since it would have to be equal to $0$ $\mu$-almost everywhere in order to be integrable.

\vspace{0.3cm}
\begin{example}
Let $\mathcal{H} \subseteq \mathcal{F}$ be a sub-$\sigma$-algebra such that $\mu\vert_\mathcal{H}$ is a $\sigma$-finite measure. Given an integrable function $X : \Omega \rightarrow \mathbb{R}$, we can define a measure $\nu$ on $(\Omega, \mathcal{F})$ via
\[
	\nu(A) := \int_A X \; \textrm{d}\mu
\]
It is easy to verify that $\mu\vert_\mathcal{H}(A) = 0$ implies $\nu\vert_\mathcal{H}(A) = 0$, i.e. $\nu\vert_\mathcal{H}$ is absolutely continuous with respect to $\mu\vert_\mathcal{H}$. Using the Radon-Nikodym Theorem, we obtain an $\mathcal{H}$-measurable function $g : \Omega \rightarrow \mathbb{R}$ such that
\[
	\nu\vert_\mathcal{H}(A) = \int_A g \;\textrm{d}\mu\vert_\mathcal{H}
\]
Thus for any $A \in \mathcal{H}$, we have
\[
	\int_A X \; \textrm{d}\mu = \int_A g \;\textrm{d}\mu\vert_\mathcal{H} = \int_A g \;\textrm{d}\mu
\]
In the second equality, we use the fact that $g$ is $\mathcal{H}$-measurable. Radon-Nikodym also guarentees that this function $g$ is unique up to a $\mu\vert_\mathcal{H}$-null set. Since all $\mu\vert_\mathcal{H}$-null sets are also $\mu$-null sets, the function $g$ satisfies the requirements of a conditional expectation.
\end{example}
\vspace{0.3cm}

Technicalities aside, this shows that the conditional expectation always exists and is unique up to $\mu$-null set for all $X \in \mathcal{L}^1(\mathbb{R})$. Our job now will be to construct a similar operator on arbitrary Banach spaces using methods from functional analysis and measure theory.

\section{Preliminaries}

In anticipation of our construction, we need to lift some results from the real setting to our more general setting. Our fundamental tool in this regard will be the \textbf{averaging theorem}. The proof of this theorem is due to Serge Lang \cite{Lang_1993}. The theorem allows us to make statements about a function's value almost everywhere, depending on the value it's integral takes on various sets of the measure space.

\subsection{Averaging Theorem}

Before we introduce and prove the averaging theorem, we will first show the following lemma which is crucial for our proof. While not stated exactly in this manner, our proof makes use of the characterization of second-countable topological spaces given in the book General Topology by Ryszard Engelking (Theorem 4.1.15) \cite{engelking_1989}.

\begin{lemma}
Let $E$ be a separable metric space. Then there exists a countable set $D \subseteq E$, such that the set of open balls
\[
	\mathcal{B} = \{ B_\varepsilon(x) \; \vert \; x \in D, \; \varepsilon \in \mathbb{Q} \cap (0, \infty) \}
\]
generates the topology on $E$. Here $B_\varepsilon(x)$ is the open ball of radius $\varepsilon$ with centre $x$.
\end{lemma}

\begin{proof}
In the context of metric spaces, second-countability is equivalent to separability. Consequently, there exists some non-empty countable subset $D \subseteq E$, which is dense in $E$. We want to show that this $D$ fulfills the statement above. For this end we will use the following equivalence which is valid for any $\mathcal{A} \subseteq \mathcal{P}(E)$

\[
	\mathcal{A} \textrm{ is topological basis} \Longleftrightarrow \forall \textrm{open } U.\; \forall x \in U.\; \exists A \in \mathcal{A}.\; x \in A \wedge A \subseteq U
\]

Let $U \subseteq E$ be open. Fix $x \in U$. Since $U$ is open and we are working with the metric topology, there is some $\varepsilon > 0$, such that $B_\varepsilon(x) \subseteq U$. Furthermore, we know that a set $D$ is dense if and only if for any non-empty open subset $O \subseteq E$, $D \cap O$ is also non-empty. Therefore, there exists some $y \in D \cap B_{\varepsilon/3}(x)$. Since $\mathbb{Q}$ is dense in $\mathbb{R}$, there exists some $r \in \mathbb{Q}$ with $e/3 < r < e/2$. It is easy to check that $x \in B_r(y)$ and $B_r(y) \subseteq U$ with $y \in D$ and $r \in \mathbb{Q} \cap (0, \infty)$. This concludes the proof.
\end{proof}

Now we are ready to state and subsequently prove the averaging theorem.

\begin{theorem} (Averaging Theorem) \par
Let $(\Omega, \mathcal{F}, \mu)$ be some $\sigma$-finite measure space. Let $f \in L^1(E)$. Let $S$ be a closed subset of $E$ and assume that for all measurable sets $A \in \mathcal{F}$ with finite and non-zero measure the following holds
\[
	\frac{1}{\mu(A)}\int_A f \;\textrm{d}\mu \in S
\]
Then $f(x) \in S$ for $\mu$-almost all $x$.
\end{theorem}
\begin{proof}
Without loss of generality we will show the statement assuming $\mu(\Omega) < \infty$. Let $v \in E$ and $v \notin S$. 

We show by contradiction that if $B_r(v) \cap S = \varnothing$,  then $A := f^{-1}(B_r(v))$, the set of all $x \in \Omega$ such that $f(x) \in B_r(v)$, is a $\mu$-null set. Assume $\mu(A) > 0$. We have

\begin{align*}
	\left\lVert \frac{1}{\mu(A)}\int_A f \;\textrm{d}\mu  - v \right\rVert &= \left\lVert \frac{1}{\mu(A)}\int_A f - v \;\textrm{d}\mu \right\rVert \\
	&\le \frac{1}{\mu(A)}\int_A \lVert f - v \rVert \;\textrm{d}\mu \\
	&< r
\end{align*}

The last inequality follows from the fact that $f(x) \in B_r(v)$ for $x \in A$. This contradicts our first assumption. Therefore $\mu(A) = 0$.

Notice that $E \setminus S$ is an open subset of $E$. By the previous lemma, there exist open balls $B_{r_i}(w_i)$ with $r_i \in \mathbb{Q}_{\ge 0}$, $w_i \in D$ for $i \in \mathbb{N}$ such that $\bigcup_i B_{r_i}(w_i) = - S$. Obviously, $w_i \in E \setminus S$ and $B_{r_i}(w_i) \cap S = \varnothing$ for $i \in \mathbb{N}$. It follows

\begin{align*}
	\mu(f^{-1}(E \setminus S)) &= \mu\left(\bigcup_i f^{-1}(B_{r_i}(w_i))\right) \\
	&\le \sum_i \mu(f^{-1}(B_{r_i}(w_i))) \\
	&= 0
\end{align*}

Thus $\{f \notin S \}$ is a $\mu$-null set, which completes the proof.

\end{proof}

At the beginning of our proof, we assumed $\mu(\Omega) < \infty$ without loss of generality. This is only possible since we assumed the measure space in question to be $\sigma$-finite. To simplify the formalization of proofs employing this argument, we have introduced the following induction scheme

\begin{isalemma}
{\small
	\begin{lstlisting}[style=isabelle]
	lemma sigma_finite_measure_induct:
	  assumes "$\bigwedge N \; \Omega. \;\; \texttt{finite\_measure} \; N$
			$\Longrightarrow N = \texttt{restrict\_space} \; M \; \Omega$
			$\Longrightarrow \Omega \in \texttt{sets} \; M$
			$\Longrightarrow \texttt{emeasure}\; N \; \Omega \neq \infty $
			$\Longrightarrow \texttt{emeasure}\; N \; \Omega \neq 0$
			$\Longrightarrow \texttt{almost\_everywhere} \; N \; Q$"
	  and "$\texttt{Measurable.pred} \; M \; Q$"
	  shows "$\texttt{almost\_everywhere} \; M \; Q$"
	  $\dots$
	\end{lstlisting}
}
\end{isalemma}

This induction scheme allows us prove results about a $\sigma$-finite measure space $M$, assuming that we can show the property on arbitrary subspaces of $M$ with finite measure. For increased usability, we include additional assumptions such as $\texttt{emeasure}\; N \; \Omega \neq 0$ which let us to avoid unnecessary trivial cases. The proof of this induction scheme is straightforward.
\begin{proof}
Let $M = (\Omega, \Sigma, \mu)$ be a $\sigma$-finite measure space. There exists a family of sets with finite measure $(\Omega_i)_{i \in \mathbb{N}}$ such that $\bigcup_{i \in \mathbb{N}} \Omega_i = \Omega$. By assumption, the property $Q$ holds $\mu$-almost everywhere on all $\Omega_i$. Therefore the sets $\Omega_i \cap \{\neg Q\} \in \Sigma\vert_{\Omega_i} \subseteq \Sigma$ are all $\mu$-null sets. This means that $\bigcup_{i \in \mathbb{N}} (\Omega_i \cap \{\neg Q\}) = \{\neg Q\}$ is also $\mu$-null set, which completes the proof.
\end{proof}

Now that we have the averaging theorem at our disposal, we can lift the following results from the real case, to our more general setting.

\begin{corollary}
	Let $f \in L^1(E)$ and $\int_A f \;\textrm{d}\mu = 0$ for all measurable sets $A \subseteq \Omega$. Then $f = 0$ $\mu$-almost everywhere. 
\end{corollary}
\begin{proof}
	Apply the averaging theorem with $S = \{0\}$.
\end{proof}

\begin{corollary} (Uniqueness of Densities) \par
	Let $f, g \in L^1(E)$ and $\int_A f \;\textrm{d}\mu = \int_A g \;\textrm{d}\mu$ for all measurable sets $A \subseteq \Omega$. Then $f = g$ $\mu$-almost everywhere. 
\end{corollary}
\begin{proof}
	Follows directly from the previous corollary.
\end{proof}

\begin{corollary}
	Let $E$ be linearly orderable. Let $f \in L^1(E)$ and $\int_A f \;\textrm{d}\mu \ge 0$ for all measurable sets $A \subseteq \Omega$. Then $f$ is non-negative $\mu$-almost everywhere. 
\end{corollary}
\begin{proof}
	Our first assumption guarantees that $\{ y \in E \;\vert\; y \ge 0 \}$ is a closed subset of $E$. Applying the averaging theorem on this set, yields the desired result.
\end{proof}

The corollary on the uniqueness of densities is crucial in showing that the conditional expectation is unique as an element of $L^1(E)$.

\subsection{Diameter Lemma}

The goal of this subsection is to prove the diameter lemma, which provides a characterization of Cauchy sequences in metric spaces.

\begin{definition}
Let $E$ be a metric space with metric $d : E \times E \rightarrow \mathbb{R}$. The diameter of a set $A$ is defined as
\[
	\textrm{diam}(A) = \sup_{x,y \in A} d(x,y)
\]
\end{definition}

Intuitively the diameter of a set $A$ measures how spread out or "large" the set $A$ is with respect to the distance defined by the metric.

\begin{lemma} (Diameter Lemma)\par
	Let $E$ be a metric space with metric $d : E \times E \rightarrow \mathbb{R}$ and $(s_i)_{i\in\mathbb{N}} \subseteq E$ a sequence. Define $S_n = \{s_i \; \vert \; i \ge n \}$. The sequence $(s_i)_{i\in\mathbb{N}}$ is Cauchy, if and only if $S_0$ is bounded and
	\[
		\lim_{n \to \infty} \textup{diam}(S_n) = 0
	\]
\end{lemma}
\begin{proof}
\;\newline
First, assume $(s_i)_{i\in\mathbb{N}}$ is Cauchy. 

Recall that a set $A$ is bounded if there exists some $x \in E$ and $\varepsilon \in \mathbb{R}$ such that $d(x,y) \le \varepsilon$ for all $y \in A$. Since $(s_i)_{i\in\mathbb{N}}$ Cauchy, there exists some $N \in \mathbb{N}$ such that $d(s_n,s_m) < 1$ for all $n, m \ge N$. The set $\{s_i \; \vert \; i \in \{0,\dots,N\}\}$ is bounded since it is finite. Thus there exists some $a \in \mathbb{R}$ such that $d(s_N, s_i) < a$ for all $i \in \{0,\dots,N\}$. Therefore $d(s_N, s_i) < \max(a,1)$ for all $i \in \mathbb{N}$, which shows that $S_0$ is bounded. 

We know $S_n \subseteq S_m$ for $n \ge m$. Therefore $\textrm{diam}(S_n) < \infty$ for all $n \in \mathbb{N}$.

Let $\varepsilon > 0$. Then there exists some $N \in \mathbb{N}$ such that $d(s_n,s_m) < \frac{\varepsilon}{2}$ for all $n, m \ge N$. Hence
\[
	\textrm{diam}(S_N) = \sup_{x,y \in S_N} d(x,y) \le \frac{\varepsilon}{2} < \varepsilon
\]
Furthermore, we have $\textrm{diam}(S_n) \le \textrm{diam}(S_N)$ for $n \ge N$ because of the subset relation stated above. Thus $\lim_{n \to \infty} \textup{diam}(S_n) = 0$.

For the other direction, assume $\lim_{n \to \infty} \textup{diam}(S_n) = 0$ and that $S_0$ is bounded. 

Hence $\textrm{diam}(S_n) < \infty$ for all $n \in \mathbb{N}$ with the same argument as above. 

Let $\varepsilon > 0$. There exists some $N \in \mathbb{N}$ such that $\sup_{x,y \in S_n} d(x,y) < \varepsilon$ for all $n \ge N$. Hence $d(x,y) < \varepsilon$ for all $x, y \in S_n$ for $n \ge N$. This implies $d(s_i,s_j) < \varepsilon$ for all $i, j \ge n \ge N$, which shows that $(s_i)_{i\in\mathbb{N}}$ is Cauchy.
\end{proof}

In our construction of the conditional expectation, we will use the diameter lemma to show that the limit of a sequence of simple functions admits a conditional expectation. In anticipation of this, we present the following lemmas concerning measurability and integrability.

\begin{isalemma}
{\small
	\begin{lstlisting}[style=isabelle]
	lemma borel_measurable_diameter: 
	  assumes "$\bigwedge x. \;x \in \texttt{space} \; M \implies \texttt{bounded} \; (\texttt{range} \; (\lambda i. \; s \; i \; x))$"
			  "$\bigwedge i. \; (s \; i) \in \texttt{borel\_measurable} \; M$"
	  shows "$(\lambda x. \; \texttt{diameter} \; \{s \; i \; x \; \vert \; i. \; n \le i \}) \in \texttt{borel\_measurable} \; M$"
	  $\dots$
  	\end{lstlisting}
}
\end{isalemma}

\begin{isalemma}
{\small
	\begin{lstlisting}[style=isabelle]
	lemma integrable_bound_diameter: 
	  assumes "$\texttt{integrable} \; M \; f$" 
			  "$\bigwedge i. \; (s \; i) \in \texttt{borel\_measurable} \; M$"
			  "$\bigwedge x \; i. \; x \in \texttt{space} \; M \implies \texttt{norm} \; (s \; i \; x) \le f \; x$"
	  shows "$\texttt{integrable} \; M \; (\lambda x. \; \texttt{diameter} \; \{s \; i \; x \; \vert \; i. \; n \le i\})$"
	  $\dots$
  	\end{lstlisting}
}
\end{isalemma}

The proofs are straightforward and depend on the measurability of the supremum function.

\subsection{Induction Schemes for Integrable Simple Functions}

In the upcoming sections of our work, we will frequently need to prove statements about integrable simple functions. For simple functions $s : \Omega \rightarrow \mathbb{R}_{\ge 0} \cup \{\infty\}$, the Isabelle theory \texttt{HOL\_Analysis.Nonnegative\_Lebesgue\_Integration} already provides an induction scheme \texttt{simple\_function\_induct}. For our purposes we extend this scheme to cover integrable simple functions $s : \Omega \rightarrow E$. Notice that a simple function $s$ is integrable if and only if $\mu(\{s \neq 0\}) < \infty$. The new induction scheme is as follows

\begin{isalemma}
{\small
	\begin{lstlisting}[style=isabelle]
	lemma integrable_simple_function_induct[case_names cong indicator add]:
	  assumes "$\texttt{simple\_function} \; M \; f$" "$\texttt{emeasure} \; M \; \{y \in \texttt{space} \; M. \; f \; y \neq 0\} \neq \infty$"
	  assumes cong: "$\bigwedge f \; g. \; \texttt{simple\_function} \; M \; f \implies \texttt{emeasure} \; M \; \{y \in \texttt{space} \; M. \; f \; y \neq 0\} \neq \infty$ 
					 $\implies \texttt{simple\_function} \; M \; g \implies \texttt{emeasure} \; M \; \{y \in \texttt{space} \; M. \; g \; y \neq 0\} \neq \infty$
					 $\implies (\bigwedge x. \; x \in \texttt{space} \; M \implies f \; x = g \; x) \implies P \; f \implies P \; g$"
	  assumes indicator: "$\bigwedge A \; y. \; A \in \texttt{sets} \; M \implies \texttt{emeasure} \; M \; A < \infty$ 
						  $\implies P \; (\lambda x. \; \texttt{indicator} \; A \; x \cdot_R y)$"
	  assumes add: "$\bigwedge f \; g. \; \texttt{simple\_function} \; M \; f \implies \texttt{emeasure} \; M \; \{y \in \texttt{space} \; M. \; f \; y \neq 0\} \neq \infty$
					$\implies \texttt{simple\_function} \; M \; g \implies \texttt{emeasure} \; M \; \{y \in \texttt{space} \; M. \; g \; y \neq 0\} \neq \infty$
					$\implies (\bigwedge z. \; z \in \texttt{space} \; M \implies \texttt{norm} \; (f \; z + g \; z) = \texttt{norm} \; (f \; z) + \texttt{norm} \; (g \; z))$
					$\implies P \; f \implies P \; g \implies P \; (\lambda x. \; f \; x + g \; x)$"
	  shows "$P \; f$"
	  $\dots$
	\end{lstlisting}
}
\end{isalemma}

The idea of the induction scheme is simple. We know $f$ can be represented $\mu$-a.e. as a finite sum $\sum_{i=1}^n \mathbf{1}_{A_i} \cdot c_i$  for some collection of measurable sets $(A_i)_{i=1,\dots,n}$ and elements $c_i \in E$. We do induction on $n$. In this sense, ``indicator'' corresponds to the induction basis, while ``add'' corresponds to the induction step. Since $f$ is representable as a finite sum $\mu$-a.e. we need the additional assumption ``cong'' to make sure $P$ is a well defined predicate on the space $L^1(E)$.

\begin{remark}
To make proving certain properties easier, we have the additional assumption $\lVert f(x) + g(x) \rVert = \lVert f(x) \rVert + \lVert g(x) \rVert$ in the induction step ``add''. It is easy to see why we can assume this without loss of generality. If we have some simple function $s = \sum_{i=1}^n \mathbf{1}_{A_i} \cdot c_i$, we can assume the sets $A_i$ to be pairwise disjoint. Thus, if $x \in A_j$ for some $j \le n$ we have $\lVert s(x) \rVert = \lVert \mathbf{1}_{A_j}(x) \cdot c_j\rVert = \sum_{i=1}^n \mathbf{1}_{A_i} \cdot \lVert c_i \rVert$.
\end{remark}

When working with an ordering on $E$, we may need to concern ourselves with non-negative simple functions. For this goal, we have the following induction scheme.

\begin{isalemma}
{\small
	\begin{lstlisting}[style=isabelle]
	lemma integrable_simple_function_induct_nn[case_names cong indicator add]:
	  assumes "$\texttt{simple\_function} \; M \; f$" "$\texttt{emeasure} \; M \; \{y \in \texttt{space} \; M. \; f \; y \neq 0\} \neq \infty$" 
			  "$\bigwedge x. \; x \in \texttt{space} \; M \longrightarrow f \; x \ge 0$"
	  assumes cong: "$\bigwedge f \; g. \; \texttt{simple\_function} \; M \; f \implies \texttt{emeasure} \; M \; \{y \in \texttt{space} \; M. \; f \; y \neq 0\} \neq \infty$ 
					 $\implies (\bigwedge x. \; x \in \texttt{space} \; M \implies f \; x \ge 0)$
					 $\implies \texttt{simple\_function} \; M \; g \implies \texttt{emeasure} \; M \; \{y \in \texttt{space} \; M. \; g \; y \neq 0\} \neq \infty$
					 $\implies (\bigwedge x. \; x \in \texttt{space} \; M \implies g \; x \ge 0)$
					 $\implies (\bigwedge x. \; x \in \texttt{space} \; M \implies f \; x = g \; x) \implies P \; f \implies P \; g$"
	  assumes indicator: "$\bigwedge A \; y. \; y \ge 0 \implies A \in \texttt{sets} \; M \implies \texttt{emeasure} \; M \; A < \infty$
						  $\implies P \; (\lambda x. \; \texttt{indicator} \; A \; x \cdot_R y)$"
	  assumes add: "$\bigwedge f \; g. \; \texttt{simple\_function} \; M \; f \implies \texttt{emeasure} \; M \; \{y \in \texttt{space} \; M. \; f \; y \neq 0\} \neq \infty$
					$\implies (\bigwedge x. \; x \in \texttt{space} \; M \implies f \; x \ge 0)$
					$\implies \texttt{simple\_function} \; M \; g \implies \texttt{emeasure} \; M \; \{y \in \texttt{space} \; M. \; g \; y \neq 0\} \neq \infty$
					$\implies (\bigwedge x. \; x \in \texttt{space} \; M \implies g \; x \ge 0)$
					$\implies (\bigwedge z. \; z \in \texttt{space} \; M \implies \texttt{norm} \; (f \; z + g \; z) = \texttt{norm} \; (f \; z) + \texttt{norm} \; (g \; z))$
					$\implies P \; f \implies P \; g \implies P \; (\lambda x. \; f \; x + g \; x)$"
	  shows "$P \; f$"
	  $\dots$
	\end{lstlisting}
}
\end{isalemma}

The induction scheme looks complicated and cumbersome, but in essence it is the same induction scheme as the previous one with the added assumption of non-negativity everywhere. The proof is also largely the same. We just need to show that the partial sums stay non-negative all the way through.

\pagebreak

\subsection{Bochner Integration on Linearly Ordered Banach Spaces}

When working with the real numbers, the following statement is easy to show.
\begin{center}
Let $f, g : \Omega \rightarrow \mathbb{R}$ be integrable and $f \ge g$ $\mu$-a.e., then $\int f\;\textrm{d} \mu \ge \int g\;\textrm{d} \mu$. 
\end{center}
In this subsection, we aim to provide similar results for functions $f, g : \Omega \rightarrow E$ with $E$ a linearly ordered Banach space. For the remainder of our discourse, a topological space $E$ is linearly ordered, if there exists a total ordering on $E$ such that the topology on $E$ and the order topology induced by the ordering coincide.

We start with the following lemma

\begin{lemma}
	Let $f \in L^1(E)$ and $f \ge 0$ $\mu$-a.e. Then $\int f \;\textrm{d}\mu \ge 0$.
\end{lemma}
\begin{proof}
	Since $f \in L^1(E)$, there exists a sequence of integrable simple functions $(s_n)_{n \in \mathbb{N}}$, such that $\lim_{n \to \infty} s_n(x) = f(x)$ $\mu$-a.e. and $\lim_{n \to \infty} \int s_n \;\textrm{d}\mu = \int f \;\textrm{d}\mu$. At first, we have no further information about $s_n$. However, since we know that $f \ge 0$ $\mu$-a.e, it follows that $f = \max(0,f)$ $\mu$-a.e. Using dominated convergence and the fact that the function $\max(0,\cdot)$ is continuous w.r.t to the order topology on $E$, we can show
\[
	\lim_{n \to \infty} \max(0, s_n(x)) = \max(0, f(x)) \; \mu\textrm{-a.e.}
\]
and
\[
	\lim_{n \to \infty} \int \max(0, s_n) \;\textrm{d}\mu = \int \max(0, f) \;\textrm{d}\mu
\]
The function $\max(0, s_n)$ is still a simple and integrable function, which has the additional property of being always non-negative. 

We will now show that if $h$ is a non-negative simple function, then $\int h \;\textrm{d}\mu \ge 0$. For this purpose, we will use the induction scheme for non-negative integrable simple functions that we proved in the previous subsection.

\paragraph{Case ``cong'':} Let $h = g$ $\mu$-a.e. and $\int g \;\textrm{d}\mu \ge 0$. It follows directly
\[
	\int h \;\textrm{d}\mu = \int g \;\textrm{d}\mu \ge 0
\]

\paragraph{Case ``indicator'':} Let $h = \mathbf{1}_A \cdot y$ for some measurable set $A$ with finite measure and $y \in E$ with $y \ge 0$. It follows directly 
\[
	\int h \;\textrm{d}\mu = \mu(A) \cdot y \ge 0
\]

\paragraph{Case ``add'':} Let $h = h_1 + h_2$ for some integrable simple functions $h_1$ and $h_2$. By the induction hypothesis, we have $\int h_i \;\textrm{d}\mu \ge 0$ for $i = 1,2$. Therefore
\[
	\int h \;\textrm{d}\mu = \int h_1 \;\textrm{d}\mu + \int h_2 \;\textrm{d}\mu \ge 0
\]

Hence, we know $\int \max(0, s_n) \;\textrm{d}\mu \ge 0$ for all $n \in \mathbb{N}$. Therefore, the same must hold for the limit $\lim_{n \to \infty} \int \max(0, s_n) \;\textrm{d}\mu = \int \max(0, f) \;\textrm{d}\mu$. Since $f = \max(0,f)$ $\mu$-a.e., we have $\int f \;\textrm{d}\mu = \int \max(0, f) \;\textrm{d}\mu$ and the statement follows.

\end{proof}
\begin{remark}
For the proof of this statement, we need the topology on $E$ to coincide with the order topology. Otherwise, we can't guarantee statements such as $(\forall i. \; x_i \ge 0) \implies \lim_{i \to \infty} x_i \ge 0$ or the continuity of the $\max$ function.
\end{remark}
This lemma entails the following corollary.
\begin{corollary}
	Let $f, g \in L^1(E)$ and $f \ge g$ $\mu$-a.e. Then $\int f\;\textrm{d} \mu \ge \int g\;\textrm{d} \mu$.
\end{corollary}

In Isabelle, we can replace the assumption $f \in L^1(E)$ with Borel measurability, since a non-integrable function has the value of its integral set to $0$ by default. The previous lemma can be stated as

{\small
\begin{lstlisting}[style=isabelle]
lemma integral_nonneg_AE_banach:
  assumes "$f \in \texttt{borel\_measurable} \; M$" and "AE$ \; x \; $in$ \; M. \; 0 \le f \; x$"
  shows "$0 \le \texttt{integral}^L \; M \; f$"
proof (cases "integrable $M \; f$") 
  $\dots$
qed
\end{lstlisting}
}

Furthermore, we have the following two corollaries, that also make use of the lemma on the uniqueness of densities.

\begin{isacorollary}
{\small
	\begin{lstlisting}[style=isabelle]
	lemma integral_nonneg_AE_eq_0_iff_AE:
	  assumes f: "$\texttt{integrable} \; M \; f$" and nonneg: "AE$ \; x \; $in$ \; M. \; 0 \le f \; x$"
	  shows "$\texttt{integral}^L \; M \; f = 0 \longleftrightarrow \; ($AE$ \; x \; $in$ \; M. \; f \; x = 0)$"
	proof 
	  assume *: "$\texttt{integral}^L \; M \; f = 0$"
	  {
		fix $A$ assume asm: "$A \in \texttt{sets} \; M$"
		have "$0 \le \texttt{integral}^L \; M \; (\lambda x. \; \texttt{indicator} \; A \; x \cdot f \; x)$" using nonneg by $\dots$
		moreover have "$\dots \le \texttt{integral}^L \; M \; f$" using nonneg by $\dots$
		ultimately have "$\texttt{set\_lebesgue\_integral} \; M \; A \; f = 0$" 
			unfolding set_lebesgue_integral_def using * by force
	  }
	  thus "AE x in M. f x = 0" by (intro density_zero f, blast)
	qed (auto simp add: integral_eq_zero_AE)
	\end{lstlisting}
}
\end{isacorollary}

\begin{isacorollary}
{\small
	\begin{lstlisting}[style=isabelle]
	lemma integral_eq_mono_AE_eq_AE:
	  assumes "$\texttt{integrable} \; M \; f$" "$\texttt{integrable} \; M \; g$" 
			  "$\texttt{integral}^L \; M \; f = \texttt{integral}^L \; M \; g$" "AE$ \; x \; $in$ \; M. \; f \; x \le g \; x$" 
	  shows "AE$ \; x \; $in$ \; M. \; f \; x = g \; x$"
	proof -
	  define $h$ where "$h = (\lambda x. \; g \; x - f \; x)$"
	  have "AE$ \; x \; $in$ \; M. \; h \; x = 0$" unfolding h_def using assms by $\dots$
	  then show ?thesis unfolding h_def by auto
	qed
	\end{lstlisting}
}
\end{isacorollary}

\section{Constructing the Conditional Expectation}

Before we can talk about \textit{the} conditional expectation, we must define what it means for a function to have \textit{a} conditional expectation. For this purpose we define the following predicate

\begin{isadefinition}
{\small
\begin{lstlisting}[style=isabelle]
	definition has_cond_exp :: "$'a \; \texttt{measure} \; \Rightarrow \; 'a \; \texttt{measure} \; \Rightarrow \; ('a \; \Rightarrow \; 'b) \; \Rightarrow \; ('a \; \Rightarrow \; 'b) \; \Rightarrow \texttt{bool}$" where 
	  "has_cond_exp $M \; F \; f \; g \; = (\forall A \in \texttt{sets} \; F. \; \int_A \; f \; \partial M = \int_A \; g \; \partial M)$
							  $\wedge \; \texttt{integrable} \; M \; f $
							  $\wedge \; \texttt{integrable} \; M \; g $
							  $\wedge \; g \in \texttt{borel\_measurable} \; F$"
\end{lstlisting}
}
\end{isadefinition}


This predicate precisely characterizes what it means for a function $f$ to have a conditional expectation $g$ w.r.t the measure $M$ and the sub-$\sigma$-algebra $F$. Now we can use Hilbert's $\epsilon$-operator, \lstinline[language=isabelle]{SOME} in Isabelle \cite{Nipkow-Paulson-Wenzel:2002}, to define \textit{the} conditional expectation, if it exists.


\begin{isadefinition}
{\small
	\begin{lstlisting}[style=isabelle]
		definition cond_exp :: "$'a \; \texttt{measure} \; \Rightarrow \; 'a \; \texttt{measure} \; \Rightarrow \; ('a \; \Rightarrow \; 'b) \; \Rightarrow \; ('a \; \Rightarrow \; 'b) \; \Rightarrow \texttt{bool}$" where 
		  "cond_exp $M \; F \; f \; = ($if$ \; \exists g. \; \texttt{has\_cond\_exp} \; M \; F \; f \; g \; $then$ \; ($SOME$ \; g. \; \texttt{has\_cond\_exp} \; M \; F \; f \; g) \; $else$ \; (\lambda \_. \; 0))$"
	\end{lstlisting}
}
\end{isadefinition}

A major advantage of defining the conditional expectation this way is that it allows us to make statements about its measurability and integrability, without needing to show existence or uniqueness.

\begin{isalemma}
{\small
	\begin{lstlisting}[style=isabelle]
	lemma borel_measurable_cond_exp: "cond_exp $M \; F \; f \; \in$ borel_measurable $F$"
	  by (metis cond_exp_def someI has_cond_exp_def borel_measurable_const)
	\end{lstlisting}
}
\end{isalemma}

\begin{isalemma}
{\small
	\begin{lstlisting}[style=isabelle]
	lemma integrable_cond_exp: "integrable $M$ (cond_exp $M \; F \; f$)"
	  by (metis cond_exp_def has_cond_expD(3) integrable_zero someI)
	\end{lstlisting}
}
\end{isalemma}

\subsection{Uniqueness}

The conditional expectation of a function is unique up to a $\mu$-null set. 

\begin{lemma}
	Let $f, g \in L^1(E)$ such that ``$\normalfont\texttt{has\_cond\_exp} \; M \; F \; f \; g$'' holds. Then 
	\par\noindent ``$\normalfont\texttt{has\_cond\_exp} \; M \; F \; f \; (\texttt{cond\_exp} \; M \; F \; f)$'' and
	\[
		\normalfont\texttt{cond\_exp} \; M \; F \; f = g \;\; \mu\texttt{-a.e.}
	\]
\end{lemma}
\begin{proof}
	The first statement follows directly from the definition of \texttt{cond\_exp}. To show $\texttt{cond\_exp} \; M \; F \; f = g \; \mu\texttt{-a.e.}$ we argue as follows. By the definition of \texttt{has\_cond\_exp} we have for any $A \in \texttt{sets} \; F$
	\[
		\int_A \; f \; \textrm{d}\mu = \int_A \; g \; \textrm{d}\mu
	\]
	and
	\[
		\int_A \; f \; \textrm{d}\mu = \int_A \; \texttt{cond\_exp} \; M \; F \; f \; \textrm{d}\mu
	\]
	Together with the lemma on the uniqueness of densities, we have 
	\par\noindent$\texttt{cond\_exp} \; M \; F \; f = g \;\; \mu\vert_F\texttt{-a.e.}$. The lemma follows from the fact that all $\mu\vert_F$-null sets are also $\mu$-null sets.
\end{proof}

Hence, we have the following succint characterization.

\begin{isalemma}
{\small
	\begin{lstlisting}[style=isabelle]
	lemma cond_exp_charact:
	  assumes "$\bigwedge A \in \texttt{sets} \; F. \; \int_A \; f \; \partial M = \int_A \; g \; \partial M$"
			  "$\texttt{integrable} \; M \; f$"
			  "$\texttt{integrable} \; M \; g$"
			  "$g \in \texttt{borel\_measurable} \; F$"
	  shows "AE$ \; x \; $in$ \; M. \; \texttt{cond\_exp} \; M \; F \; f \; x = g \; x$"
	  by (intro has_cond_exp_charact has_cond_expI' assms) auto
	\end{lstlisting}
}
\end{isalemma}

\subsection{Existence}

Showing the existence is a bit more involved. Specifically, what we aim to show is that $\normalfont\texttt{has\_cond\_exp} \; M \; F \; f \; (\texttt{cond\_exp} \; M \; F \; f)$ holds for any Bochner integrable $f$. We will use the standard machinery of measure theory. First, we will prove existence for indicator functions. Then we will extend our proof by linearity to simple functions. Finally we use a limiting argument to show that the conditional expectation exists for all Bochner integrable functions.

The conditional expectation operator has already been formalized for real valued functions by S\`ebastien Gou\"ezel via the definition \texttt{real\_cond\_exp}. The following lemmas show that our definition characterizes the same operator, at least in the real case.

\begin{isalemma}
{\small
	\begin{lstlisting}[style=isabelle]
lemma has_cond_exp_real:
  assumes "$\texttt{integrable} \; M \; f$"
  shows "$\texttt{has\_cond\_exp} \; M \; F \; f \; (\texttt{real\_cond\_exp} \; M \; F \; f)$"
  by (intro has_cond_expI', auto intro!: real_cond_exp_intA assms)
	\end{lstlisting}
}
\end{isalemma}

\begin{isalemma}
{\small
	\begin{lstlisting}[style=isabelle]
lemma cond_exp_real:
  assumes "$\texttt{integrable} \; M \; f$"
  shows "AE$ \; x \; $in$ \; M. \; \texttt{cond\_exp} \; M \; F \; f \; x = \texttt{real\_cond\_exp} \; M \; F \; f \; x$" 
  using has_cond_exp_charact has_cond_exp_real assms by blast
	\end{lstlisting}
}
\end{isalemma}

We can now show that the conditional expectation of indicator functions exist.

\begin{lemma}
	Let $A \subseteq \Omega$ be measurable with $\mu(A) < \infty$ and $y \in E$. Then
	\[
		\normalfont\texttt{has\_cond\_exp} \; M \; F \; (\mathbf{1}_A \cdot y) \; ((\texttt{real\_cond\_exp} \; M \; F \; \mathbf{1}_A) \cdot y)
	\]
\end{lemma}
\begin{proof}
	The statement follows directly from the linearity of the Bochner integral and the previous lemmas.
\end{proof}

Next, we show the following lemma concerning the sum of two conditional expectations.

\begin{lemma}
	Assume $\normalfont\texttt{has\_cond\_exp} \; M \; F \; f \; f'$ and $\normalfont\texttt{has\_cond\_exp} \; M \; F \; g \; g'$. Then
	\[
		\normalfont\texttt{has\_cond\_exp} \; M \; F \; (f + g) \; (f' + g')
	\]
\end{lemma}
\begin{proof}
	The statement follows directly from the linearity of the Bochner integral.
\end{proof}

Now, we can show that the conditional expectation of integrable simple functions exist.

\begin{isalemma}
{\small
	\begin{lstlisting}[style=isabelle]
corollary has_cond_exp_simple:
  assumes "$\texttt{simple\_function} \; M \; f$" "$\texttt{emeasure} \; M \; \{y \in \texttt{space} \; M. \; f \; y \neq 0\} \neq \infty$"
  shows "$\normalfont\texttt{has\_cond\_exp} \; M \; F \; f \; (\texttt{cond\_exp} \; M \; F \; f)$"
  using assms
proof (induction rule: integrable_simple_function_induct)
  case (cong f g)
  then show ?case using has_cond_exp_cong by (metis (no_types, opaque_lifting)
						Bochner_Integration.integrable_cong has_cond_expD(2)
						has_cond_exp_charact(1))
next
  case (indicator A y)
  then show ?case using has_cond_exp_charact[OF has_cond_exp_indicator] by fast
next
  case (add u v)
  then show ?case using has_cond_exp_add has_cond_exp_charact(1) by blast
qed
	\end{lstlisting}
}
\end{isalemma}

Now comes the most difficult part. Given a convergent sequence of integrable simple functions $(s_n)_{n \in \mathbb{N}}$, we must show that the sequence $(\texttt{cond\_exp} \; M \; F \; s_n)_{n \in \mathbb{N}}$ is also convergent. Furthermore, we must show that this limit satisfies the properties of a conditional expectation. Unfortunately, we will only be able to show that this sequence convergence in $L^1$. Luckily, this is enough to show that the operator $\texttt{cond\_exp} \; M \; F$ preserves limits as a function $L^1(E) \rightarrow L^1(E)$. We need the following lemma for this purpose

\begin{lemma} (Contractivity for Simple Functions) \par
	Let $f : \Omega \rightarrow E$ be an integrable simple function. Then
	\[
		\normalfont\lVert \texttt{cond\_exp} \; M \; F \; s \rVert \le \texttt{cond\_exp} \; M \; F \; (\lambda x. \lVert s \; x \rVert)
	\]
\end{lemma}
\begin{proof}
	In the real case, one can show this property by decomposing a function into positive and negative parts. The statement follows via the induction scheme \par\noindent\texttt{integrable\_simple\_function\_induct}. 
\end{proof}

The following lemma is the most involved result of our formalization.

\begin{lemma}
	Let $f : \Omega \rightarrow E$ be an integrable function. Let $(s_n)_{n \in \mathbb{N}}$ be a sequence of integrable simple functions, such that $\lim_{n \to \infty} s_n(x) = f(x)$ and $\forall n. \;\lVert s_n(x) \rVert \le 2 \cdot \lVert f(x) \rVert$ for $\mu$-almost all $x$. Then there exists some subsequence $(s_{r_n})_{n \in \mathbb{N}}$ such that
	\[
		\normalfont (\texttt{cond\_exp} \; M \; F \; s_{r_n})_{n \in \mathbb{N}} \; \textrm{is Cauchy} \; \mu\textrm{-a.e.}
	\]
	and
	\[
		\normalfont \texttt{has\_cond\_exp} \; M \; F \; f \; (\lim_{n \to \infty} \texttt{cond\_exp} \; M \; F \; s_{r_n})
	\]
\end{lemma}
\begin{proof}
	The sequence $(s_n)_{n \in \mathbb{N}}$ is Cauchy $\mu$-a.e. Hence $\lim_{n \to \infty} \textrm{diam}(S_n(x)) = 0$, with $S_n(x) := \{s_i(x) \; \vert \; i \ge n \}$ by the diameter lemma. Furthermore
	\[
		\lVert\textrm{diam}(S_n(x))\rVert \le 4 \cdot \lVert f(x) \rVert \; \mu\textrm{-a.e.}
	\]
	using the triangle inequality and our second assumption. We have already shown that $\textrm{diam}(S_n(x))$ is measurable. We apply the dominated convergence theorem and get
	\[
		\lim_{n \to \infty} \int \textrm{diam}(S_n(x)) \; \textrm{d}\mu = 0
	\]
	We will now show that $(\texttt{cond\_exp} \; M \; F \; s_n)_{n \in \mathbb{N}}$ is Cauchy in the $L^1$ norm.
	Let $\varepsilon > 0$. Hence there is some $N \in \mathbb{N}$ such that $\int \textrm{diam}(S_n(x)) < \varepsilon$. Thus for any $i,j \ge N$, we have
	\[
		\int \lVert s_i(x) - s_j(x) \rVert \; \textrm{d}\mu \le \int \textrm{diam}(S_N(x)) \; \textrm{d}\mu < \varepsilon
	\]
	by the monotonicity of the integral. Furthermore
	\begin{align*}
		&\quad\;\int \lVert (\texttt{cond\_exp} \; M \; F \; s_i)(x) - (\texttt{cond\_exp} \; M \; F \; s_j)(x) \rVert \; \textrm{d}\mu \\
		&= \int \lVert (\texttt{cond\_exp} \; M \; F \; (s_i - s_j))(x) \rVert \; \textrm{d}\mu \\
		&\le \int (\texttt{cond\_exp} \; M \; F \; (\lambda x.\; \lVert s_i(x) - s_j(x)\rVert))(x) \; \textrm{d}\mu \\
		&= \int \lVert s_i(x) - s_j(x)\rVert \; \textrm{d}\mu \\
		&< \varepsilon
	\end{align*}
	since $s_i(x)-s_j(x)$ is an integrable simple function and conditional expectation already exists in the real setting. Hence $(\texttt{cond\_exp} \; M \; F \; s_n)_{n \in \mathbb{N}}$ is Cauchy in the $L^1$ norm. Therefore, there exists some subsequence $(\texttt{cond\_exp} \; M \; F \; s_{r_n})_{n \in \mathbb{N}}$ that convergences $\mu$-a.e. We have for all $n \in \mathbb{N}$
	\[
		\lVert (\texttt{cond\_exp} \; M \; F \; s_{r_n}) (x) \rVert \le \texttt{cond\_exp} \; M \; F \; (\lambda x. \; 2 \cdot \lVert f(x)\rVert) \; \mu\textrm{-a.e.}
	\]
	Together with the dominated convergence theorem, this implies that \par\noindent$\lim_{n \to \infty} (\texttt{cond\_exp} \; M \; F \; s_{r_n})$ is integrable. \par\noindent As the limit of $F$-measurable functions, $\lim_{n \to \infty} (\texttt{cond\_exp} \; M \; F \; s_{r_n})$ is also $F$-measurable. Finally, we have for $A \in \texttt{sets} \; F$
	\begin{align*}
		\int_A (\lim_{n \to \infty} (\texttt{cond\_exp} \; M \; F \; s_{r_n}))(x) \; \textrm{d}\mu &= \lim_{n \to \infty} \int_A (\texttt{cond\_exp} \; M \; F \; s_{r_n})(x) \; \textrm{d}\mu \\
		&= \lim_{n \to \infty} \int_A s_{r_n}(x) \; \textrm{d}\mu \\
		&= \int_A f(x) \; \textrm{d}\mu 
	\end{align*}
	In the first and last equality we have again used the dominated convergence theorem. The statement follows from the definition of \texttt{has\_cond\_exp}.
\end{proof}

At one point in the proof of our lemma, we have used the fact that a convergent sequence in $L^1$ admits a subsequence which is convergent in the underlying norm $\mu$-a.e. This result is stated in Isabelle as follows

{\small
	\begin{lstlisting}[style=isabelle]
	proposition tendsto_L1_AE_subseq:
	  fixes u :: "$\texttt{nat} \Rightarrow 'a \Rightarrow 'b$"
	  assumes "$\bigwedge n. \; \texttt{integrable} \; M \; (u \; n)$"
		  and "$(\lambda n. \; (\int \; \texttt{norm} \;(u \; n \; x) \; \partial M)) \longrightarrow 0$"
	  shows "$\exists r \;:: \; \texttt{nat} \Rightarrow \texttt{nat}. \; \texttt{strict\_mono} \; r \wedge ($AE$\; x \; $in$ \; M. \; (\lambda n. \; u \; (r \; n) \; x) \longrightarrow 0)$"
	  $\dots$
	\end{lstlisting}
}

In our case, we can't easily formulate the convergence of $(\texttt{cond\_exp} \; M \; F \; s_n)_{n \in \mathbb{N}}$ in the $L^1$ norm in the manner stated above. Therefore we have introduced the following lemma which is more flexible. Mathematically, the underlying argument is the same.

\begin{isalemma}
{\small
	\begin{lstlisting}[style=isabelle]
	lemma cauchy_L1_AE_cauchy_subseq:
	  fixes s :: "$\texttt{nat} \Rightarrow 'a \Rightarrow 'b$"
	  assumes "$\bigwedge n. \; \texttt{integrable} \; M \; (s \; n)$"
		  and "$\bigwedge e. \; e > 0 \implies \exists N. \; \forall i\ge N. \forall j \ge N. \; $LINT$ \; x\vert M. \; \texttt{norm} \; (s \; i \; x \; - \; s \; j \; x) < e$"
	  obtains $r$ where "$\texttt{strict\_mono} \; r$" "AE$ \; x \; $in$ \; M. \; \texttt{Cauchy} \; (\lambda i. \; s \; (r \; i) \; x)$"
	  $\dots$
	\end{lstlisting}
}
\end{isalemma}


The main result of this subsection is formalized in Isabelle as follows

\begin{isacorollary}
{\small
	\begin{lstlisting}[style=isabelle]
	corollary has_cond_expI:
	  assumes "$\texttt{integrable} \; M \; f$"
	  shows "$\texttt{has\_cond\_exp} \; M \; F \; f \; (\texttt{cond\_exp} \; M \; F \; f)$"
	proof -
	  obtain s where s_is: "$\bigwedge i. \; \texttt{simple\_function} \; M \; (s \; i)$" 
	  					   "$\bigwedge i. \; \texttt{emeasure} \; M \; \{y \in \texttt{space} \; M. \; s \; i \; y \neq 0\} \neq \infty$" 
						   "$\bigwedge x. \; x \in \texttt{space} \; M \implies (\lambda i. \; s \; i \; x) \longrightarrow f \; x$" 
						   "$\bigwedge x \; i. \; x \in \texttt{space} \; M \implies \; \texttt{norm} \; (s \; i \; x) \le 2 \cdot \texttt{norm} \; (f \; x)$" 
				   using integrable_implies_simple_function_sequence[OF assms] by blast
	  show ?thesis using has_cond_exp_simple_lim[OF assms s_is] has_cond_exp_charact(1) by metis
	qed
	\end{lstlisting}
}
\end{isacorollary}

\subsection{Properties of the Conditional Expectation}

\subsubsection{Identity on $F$-measurable functions}

If an integrable function $f$ is already $F$-measurable, then $\texttt{cond\_exp} \; M \; F \; f = f \;\;\mu\texttt{-a.e.}$ This is a corollary of the lemma on the characterization of \texttt{cond\_exp}.
\begin{isacorollary}
{\small
	\begin{lstlisting}[style=isabelle]
	corollary cond_exp_F_meas:
	  assumes "$\texttt{integrable} \; M \; f$"
			  "$f \in \texttt{borel\_measurable} \; F$"
	  shows "AE$ \; x \; $in$ \; M. \; \texttt{cond\_exp} \; M \; F \; f \; x = f \; x$"
	  by (rule cond_exp_charact, auto intro: assms)
	\end{lstlisting}
}
\end{isacorollary}

\subsubsection{Tower Property}

The following property is called the \textit{tower property} of the conditional expectation.

\begin{lemma}
	Let $F$ and $G$ be nested sub-$\sigma$-algebras, i.e. $F \subseteq G \subseteq \Sigma$. Then, for any $f \in L^1(E)$, we have
	\[
		\normalfont \texttt{cond\_exp} \; M \; F \; (\texttt{cond\_exp} \; M \; G \; f) = \texttt{cond\_exp} \; M \; F \; f \; \mu\textrm{-a.e.}
	\]
\end{lemma}
\begin{proof}
	For any $A \in F$, we have
	\begin{align*}
		\int_A \texttt{cond\_exp} \; M \; G \; f \; \textrm{d} \mu &= \int_A f \; \textrm{d} \mu \\
		&= \int_A \texttt{cond\_exp} \; M \; F \; f \; \textrm{d} \mu
	\end{align*}
	since $A$ is also in $G$. The characterization lemma yields the result.
\end{proof}

\subsubsection{Contractivity}

A linear operator $L : V \rightarrow W$ between normed vector spaces $V$ and $W$ is called a \textit{contraction} if its operator norm
\[
	\lVert L \rVert_{\texttt{op}} = \inf\{ c \geq 0 : \lVert Lv \rVert_W \leq c \lVert v \rVert_V \textrm{ for all } v \in V \}
\]
is less than or equal to 1. Such an operator always preserves limits and has other useful properties in functional analysis \cite{sznagy2010}.

\begin{lemma} (Contractivity) \par
	Let $f \in L^1(E)$. Then
	\[
		\normalfont\lVert \texttt{cond\_exp} \; M \; F \; f \rVert \le \texttt{cond\_exp} \; M \; F \; (\lambda x. \lVert f(x) \rVert)
	\]
\end{lemma}
\begin{proof}
	We have already shown contractivity in the case of simple functions. Since $f$ is integrable, there exists a sequence of simple functions $(s_n)_{n \in \mathbb{N}}$ such that
	\[
		\lim_{n \rightarrow \infty} s_n= f \; \mu\textrm{-a.e.}
	\]
	and
	\[
		\lVert s_n(x) \rVert \le 2 \cdot \lVert f(x) \rVert \; \mu\textrm{-a.e. for all } n \in \mathbb{N}
	\]
	Using the results of the previous subsection, we obtain a subsequence $(s_{r_n})_{n \in \mathbb{N}}$ such that
	\[
		\lim_{n \rightarrow \infty} (\texttt{cond\_exp} \; M \; F \; s_{r_n}) = \texttt{cond\_exp} \; M \; F \; f \quad \mu\textrm{-a.e.}
	\]
	With the exact same arguments applied to the sequence of simple functions $(\lambda x. \lVert s_{r_n}(x) \rVert)_{n \in \mathbb{N}}$, we obtain a sub-subsequence $(s_{r_{r'_n}})_{n \in \mathbb{N}}$ such that
	\[
		\lim_{n \rightarrow \infty} (\texttt{cond\_exp} \; M \; F \; (\lambda x. \lVert s_{r_{r'_n}}(x) \rVert)) = \texttt{cond\_exp} \; M \; F \; (\lambda x. \lVert f(x)\rVert) \quad\mu\textrm{-a.e.}
	\]
	Furthermore, we have
	\[
		\lVert (\texttt{cond\_exp} \; M \; F \; s_{r_{r'_n}})(x) \rVert \le (\texttt{cond\_exp} \; M \; F \; (\lambda x. \lVert s_{r_{r'_n}}(x) \rVert))(x) \quad\mu\textrm{-a.e.}
	\]
	for all $n \in \mathbb{N}$, since the functions in question are simple. Taking the limits on both sides and using the continuity of the norm yields the result.
\end{proof}

\begin{corollary}
	The linear operator $\normalfont\texttt{cond\_exp} \; M \; F : L^1(E) \rightarrow L^1(E)$ is a contraction.
\end{corollary}
\begin{proof}
	Let $f \in L^1(E)$. From the previous lemma we have 
	\begin{align*}
		\lVert \texttt{cond\_exp} \; M \; F \; f \rVert_1 &= \int \lVert \texttt{cond\_exp} \; M \; F \; f \rVert \; \textrm{d} \mu \\
		&\le \int \texttt{cond\_exp} \; M \; F \; (\lambda x. \lVert f(x) \rVert) \; \textrm{d} \mu \\
		&= \int \lVert f \rVert \textrm{d} \mu = \lVert f \rVert_1\\
	\end{align*}
	Hence $\lVert \texttt{cond\_exp} \; M \; F \rVert_{\texttt{op}} \le 1$
\end{proof}

\subsubsection{Pulling Out What's Known}

The following property of the conditional expectation is called ``pulling out what's known''. 

\begin{lemma}
	Let $f : \Omega \rightarrow \mathbb{R}$ be an $F$-measurable function. Let $g \in L^1(E)$ and $f \cdot g \in L^1(E)$. Then
	\[
		\normalfont \texttt{cond\_exp} \; M \; F \; (f \cdot g) = f \cdot \texttt{cond\_exp} \; M \; F \; g \quad \mu\textrm{-a.e.}
	\]
\end{lemma}
\begin{proof}
	The proof of this lemma is involved as well. Therefore we will only focus on the core idea of the proof. We will also assume that the result already holds in the real setting. We show the following seemingly less general statement for $z : \Omega \rightarrow \mathbb{R}$ $F$-measurable and $z \cdot g \in L^1(E)$:
	\[
		\int z \cdot g \; \textrm{d} \mu = \int z \cdot \texttt{cond\_exp} \; M \; F \; g \; \textrm{d}
	\]
	The result will follow by taking $z = f \cdot \mathbf{1}_A$ for $A \in F$.
	Since $z$ is measurable, there exists some sequence of simple functions $(s_n)_{n \in \mathbb{N}}$ such that
	\[
		\lim_{n \rightarrow \infty} s_n= z \; \mu\textrm{-a.e.}
	\]
	and
	\[
		\lvert s_n(x) \rvert \le 2 \cdot \lvert z(x) \rvert \; \mu\textrm{-a.e. for all } n \in \mathbb{N}
	\]
	In this case one can easily check that
	\[
		\int s_n \cdot g \; \textrm{d} \mu = \int s_n \cdot \texttt{cond\_exp} \; M \; F \; g \; \textrm{d}
	\]
	for all $n \in \mathbb{N}$
	
	By our additional assumption that the result already holds in the real case, we have
	\[
	\lvert z \cdot \texttt{cond\_exp} \; M \; F \; (\lambda x. \lVert g(x \rVert))\rvert = \texttt{cond\_exp} \; M \; F \; (\lambda x. \lvert z(x) \cdot \lVert g(x)\rVert\rvert)
	\]
	Using the contractivity of the conditional expectation and the above bound on $s_n$, it follows that 
	\[
		\lVert s_n \cdot \texttt{cond\_exp} \; M \; F \; g \rVert \le 2 \cdot \texttt{cond\_exp} \; M \; F \; (\lambda x. \lvert z(x) \cdot \lVert g(x)\rVert\rvert)
	\]
	Applying the dominated convergence theorem twice, we get
	\[
		\lim_{n \rightarrow \infty} \int s_n \cdot g \; \textrm{d} \mu = \int z \cdot g \; \textrm{d} \mu
	\]
	and
	\[
		\lim_{n \rightarrow \infty} \int s_n \cdot \texttt{cond\_exp} \; M \; F \; g \; \textrm{d} \mu = \int z \cdot \texttt{cond\_exp} \; M \; F \; g \; \textrm{d} \mu
	\]
	Since the sequence on the right hand side are equal, the statement follows from the fact that limits are unique.
\end{proof}

\section{Conditional Expectation on Linearly Ordered Banach Spaces}

In the presence of an ordering, we can prove certain monotonicity properties of the conditional expectation. We start with the following two lemmas

\begin{lemma}
	Let $f \in L^1(E)$. Assume $f \ge c$ $\mu$-a.e. for some $c \in E$. Then 
	\[
		\normalfont\texttt{cond\_exp} \; M \; F \; f \ge c \quad \mu\textrm{-a.e.}
	\]
\end{lemma}
\begin{proof}
	We will show the statement using the averaging theorem. Let $A \in F$ be a measurable set with $\mu(A) < \infty$. Then
	\begin{align*}
		c &= \frac{1}{\mu(A)} \int_A c \; \textrm{d} \mu \\
		&\le \frac{1}{\mu(A)} \int_A f \; \textrm{d} \mu \\
		&= \frac{1}{\mu(A)} \int_A \texttt{cond\_exp} \; M \; F \; f \; \textrm{d} \mu \\
		&= \frac{1}{\mu(A)} \int_A \texttt{cond\_exp} \; M \; F \; f \; \textrm{d} \mu\vert_F \\
	\end{align*}
	Hence $\int_A \texttt{cond\_exp} \; M \; F \; f \; \textrm{d} \mu\vert_F \in \{x \in E \;\vert\; x \ge c \}$. The statement follows from the fact that $\{x \in E \;\vert\; x \ge c \}$ is closed.
\end{proof}

\begin{lemma}
	Let $f \in L^1(E)$. Assume $f > c$ $\mu$-a.e. for some $c \in E$. Then 
	\[
		\normalfont\texttt{cond\_exp} \; M \; F \; f > c \quad \mu\textrm{-a.e.}
	\]
\end{lemma}
\begin{proof}
	The averaging theorem is not applicable in this case since $\{x \in E \;\vert\; x > c \}$ is not closed. Therefore, we argue as follows.
	
	Let $S = \{\texttt{cond\_exp} \; M \; F \; f \le c\}$. The conditional expectation $\texttt{cond\_exp} \; M \; F \; f$ is $F$-measurable, hence $S \in F$. Since $F$ is a $\sigma$-finite sub-$\sigma$-algebra, we can assume without loss of generality that $\mu(S) < \infty$.
	The assumption $f > c$ $\mu$-a.e. implies
	\[
		\int_S f \; \textrm{d} \mu \ge \int_S c \; \textrm{d} \mu
	\]
	Furthermore, by the definition of $S$
	\begin{align*}
		\int_S c \; \textrm{d} \mu &\ge \int_S \texttt{cond\_exp} \; M \; F \; f \; \textrm{d} \mu \\
		&= \int_S f \; \textrm{d} \mu
	\end{align*}
	Hence $\int_S f \; \textrm{d} \mu = \int_S c \; \textrm{d} \mu$. By Corollary 3.1.16, we have
	\[
		\mathbf{1}_S \cdot f = \mathbf{1}_S \cdot c \quad \mu\textrm{-a.e.}
	\]
	Because of our assumption $f > c$ $\mu$-a.e., this can only be the case if $S$ is a $\mu$-null set, which completes the proof.
\end{proof}

The corresponding lemmas for $(\le)$ and $(<)$ are simple corollaries.

\begin{isacorollary}
{\small
	\begin{lstlisting}[style=isabelle]
	corollary cond_exp_le_c:
	  assumes "$\texttt{integrable} \; M \; f$" "AE$ \; x \; $in$ \; M. \; f \; x \le c$"
	  shows "AE$ \; x \; $in$ \; M. \; \texttt{cond\_exp} \; M \; F \; f \; x \le c$"
	  $\dots$
	\end{lstlisting}
}
\end{isacorollary}

\begin{isacorollary}
{\small
	\begin{lstlisting}[style=isabelle]
	corollary cond_exp_less_c:
	  assumes "$\texttt{integrable} \; M \; f$" "AE$ \; x \; $in$ \; M. \; f \; x < c$"
	  shows "AE$ \; x \; $in$ \; M. \; \texttt{cond\_exp} \; M \; F \; f \; x < c$"
	  $\dots$
	\end{lstlisting}
}
\end{isacorollary}

Finally, we can demonstrate the operator's monotonicity and that it preserves the pointwise maximum and minimum of two integrable functions.

\begin{isacorollary}
{\small
	\begin{lstlisting}[style=isabelle]
	corollary cond_exp_mono:
	  assumes "$\texttt{integrable} \; M \; f$" "$\texttt{integrable} \; M \; g$" "AE$ \; x \; $in$ \; M. \; f \; x \le g \; x$"
	  shows "AE$ \; x \; $in$ \; M. \; \texttt{cond\_exp} \; M \; F \; f \; x \le \texttt{cond\_exp} \; M \; F \; g \; x$"
	  using cond_exp_le_c[OF Bochner_Integration.integrable_diff, OF assms(1,2), of 0] 
			cond_exp_diff[OF assms(1,2)] assms(3) by auto
	\end{lstlisting}
}
\end{isacorollary}

\begin{isacorollary}
{\small
	\begin{lstlisting}[style=isabelle]
	corollary cond_exp_max:
	  assumes "$\texttt{integrable} \; M \; f$" "$\texttt{integrable} \; M \; g$"
	  shows "AE$ \; x \; $in$ \; M. \; \texttt{cond\_exp} \; M \; F \; (\lambda x. \; \max \; (f \; x) \; (g \; x)) \; x = \max \; (\texttt{cond\_exp} \; M \; F \; f \; x) \; (\texttt{cond\_exp} \; M \; F \; g \; x)$"
	  $\dots$
	\end{lstlisting}
}
\end{isacorollary}

\begin{isacorollary}
{\small
	\begin{lstlisting}[style=isabelle]
	corollary cond_exp_min:
	  assumes "$\texttt{integrable} \; M \; f$" "$\texttt{integrable} \; M \; g$"
	  shows "AE$ \; x \; $in$ \; M. \; \texttt{cond\_exp} \; M \; F \; (\lambda x. \; \min \; (f \; x) \; (g \; x)) \; x = \min \; (\texttt{cond\_exp} \; M \; F \; f \; x) \; (\texttt{cond\_exp} \; M \; F \; g \; x)$"
	  $\dots$
	\end{lstlisting}
}
\end{isacorollary}

Apart from some auxillary lemmas that we left out on purpose, this wraps up our overview of the formalization of the conditional expectation operator.
